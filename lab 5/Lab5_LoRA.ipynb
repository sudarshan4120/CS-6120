{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f603564b-538e-4b59-be36-70c7cbb2fb75",
   "metadata": {},
   "source": [
    "## Lab 5 (Part 1) - Low Rank Adaptation\n",
    "---\n",
    "In this lab, we will implement low-rank adaptation from scratch for one of the Linear (fully connected feedforward) layers of an image classification neural network. The underlying mechanism is much the same for LLMs; however a small image classifier is more tractable to work with in class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50478d4-51a1-4e62-852d-3056712d2d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347f0586-33c7-4235-a8e6-06a9ba57462e",
   "metadata": {},
   "source": [
    "### MNIST Digit Recognition \n",
    "Let us first build a convolutional neural network to classify images from the MNIST handwritten digits dataset.\n",
    "\n",
    "In your network, the architecture should include two convolutional (Conv2d) layers, each followed by a relu activation. After *both* of these pairs of operations are completed, use a max-pooling (max_pool2d), followed by a dropout (dropout1). Then flatten the data, and pass it through two fully connected layers, with the first of the two having 2048 output neurons, and the second having 10 output neurons for our classification. Finally, return the log softmax of the outputs.\n",
    "\n",
    "Feel free to experiment with layer sizes, kernel sizes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1376ffd1-7d1b-406b-9307-a6c350bdfa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ''' YOUR CODE HERE'''\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de98219-5cd6-4bb8-be9c-0ffb0901b214",
   "metadata": {},
   "source": [
    "The training and testing functions are provided below for you. We use the negative log likelihood loss in this implementation.\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
    "\n",
    "Feel free to make changes if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e04177e-d374-4651-a2b8-f3b3001bd2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0c6cd9-6d7d-41b9-832c-44ee95519576",
   "metadata": {},
   "source": [
    "Next, we check for CUDA/MPS availability, and set the appropriate device as default. We then normalize all images as \n",
    "\n",
    "output[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "\n",
    "After initializing the dataloaders, we then create an instance of our model, set the optimizer, set up a learning rate scheduler, and call the training/testing functions for a few epochs. Finally, we save the model weights for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157e0d37-f947-4236-85e8-e8969fe235cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Known mean and std dev of MNIST Dataset\n",
    "    ])\n",
    "\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                   transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,batch_size=64)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=64)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.01)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "for epoch in range(1, 10 + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b743f495-8ddd-481a-989b-22ade066bc29",
   "metadata": {},
   "source": [
    "### Fine Tuning for a different dataset\n",
    "---\n",
    "Now imagine we wish to use the same neural network, but on the [FashionMNIST dataset](https://github.com/zalandoresearch/fashion-mnist) instead, but to also retain the ability to classify MNIST digits that we have just gained. \n",
    "\n",
    "Using low-rank adaptation (LoRA) serves a dual purpose; first, we can learn a new set of weights, which act as a separate 'add-on' for the new task, and second, we can get away with learning fewer weights instead of fine-tuning the entire MNIST model. To get a sense of the compute savings, print out the number of parameters in the original model in the code block below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dadd390-bc71-4fed-bbf9-c845f4745bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdada693-a050-4289-88b2-bde4058503b4",
   "metadata": {},
   "source": [
    "Now, let us load the new dataset, and the corresponding dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6fb146-b9da-492b-bfd5-7005278a86a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = datasets.FashionMNIST('../data', train=True, download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.FashionMNIST('../data', train=False,\n",
    "                   transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04ec189-859a-4b7d-a9d8-ec4227a13a3a",
   "metadata": {},
   "source": [
    "LoRA is typically used with fully connected layers (or self-attention layers, where everything is interconnected). In our case, we will use LoRA as an additive set of weights to the *first fully connected layer* in our neural network. Let the weights of this layer be represented by the matrix W.\n",
    "\n",
    "We need to find a proxy set of matrices A and B, such that $\\Delta$W = AB (approximately). If W has dimension m X n, then A has dimension m X r, and B has dimension r X n, where r is the rank of the weight matrix W. Let us start by calculating the rank of the matrix W below. Experiment with the 'rtol' argument to see how the rank changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8dc8ae-4114-4411-9c3e-d42b70e4d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc57d560-fb81-45a7-87c4-59eddb2d8330",
   "metadata": {},
   "source": [
    "Now initialize A and B to be matrices of the appropriate size in the block below, ensuring that A contains random values sampled from a normal distribution with a mean of 0 and a variance of 1, and B contains all zeros. Make sure their requires_grad flag is set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fc7320-dbbe-43ea-beee-f7e8ea152ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505ed9a1-f29c-4d7c-a80c-3051127794c7",
   "metadata": {},
   "source": [
    "How many parameters are we learning with this approach? How does this compare to the original linear layer's number of weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7229d449-405a-4252-bb08-8f89315a265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d82c39-2432-481b-8342-9ad1e9ac8476",
   "metadata": {},
   "source": [
    "Now, before we can use these new weight matrices, we need to add some functionality to our model definition to enable this! We need to obtain the model's values just before the first fully connected layer (so that we can pass them through our new weights separately), as well as isolate the remainder of the network to use with the combination -> W * values + B * (A * values). Copy the constructor and forward pass definitions from earlier in the lab, and complete the two new functions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b18ce0-08fb-4872-b4e0-556a607e8e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ''' YOUR CODE HERE'''\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' YOUR CODE HERE'''\n",
    "\n",
    "    def lora_inputs(self, x):\n",
    "        '''YOUR CODE HERE'''\n",
    "\n",
    "    def remaining_forward_pass(self,x):\n",
    "        '''YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2466a6-e380-4c37-8efe-27ff81e1c15e",
   "metadata": {},
   "source": [
    "Before we go ahead and train our new weights, let us check how well the model trained on MNIST digits performs on the FashionMNIST classification task:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fa780b-e46f-4723-9afe-fc0c70d4f52d",
   "metadata": {},
   "source": [
    "#model = Net().to(device)\n",
    "#model.load_state_dict(torch.load(\"mnist_cnn.pt\"))\n",
    "'''YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd807345-568f-4ef3-a314-cc91b9ce10e0",
   "metadata": {},
   "source": [
    "Not great! Let's finally go ahead and learn A and B with a few epochs of tuning over the new dataset. First, freeze the original model's weights, since we are not going to change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c66d01-e753-4fa9-99df-2420cb0b0bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0cd8ed-e5c3-486c-ac6b-a79f19c5ff02",
   "metadata": {},
   "source": [
    "Modify the training logic from the previous given functions to fill in the following, but now including A and B as well to get the model's outputs, using the new functions we added to our model definition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9896121b-8f6f-4e8e-b218-029ddc994ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lora(model, device, train_loader, optimizer, epoch, A, B):\n",
    "    '''YOUR CODE HERE'''\n",
    "\n",
    "def test_lora(model, device, test_loader, A, B):\n",
    "    '''YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa6aed8-677a-453f-85b0-dd4ab1701abf",
   "metadata": {},
   "source": [
    "Let's now test how well we did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc303aa-5b46-43fb-a1c5-8e32f2a5feca",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adadelta([A,B], lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "for epoch in range(1,3):\n",
    "    train_lora(model, device, train_loader, optimizer, epoch, A, B)\n",
    "    test_lora(model, device, test_loader, A, B)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490a15b3-ce8c-4dd9-a294-ec6f781e385d",
   "metadata": {},
   "source": [
    "As you can see, we were able to make huge improvements from where we started, with a much smaller number of parameters needing to be trained - not only in comparison to the entire original network, but also in comparison to just the first linear layer. In LLMs, as you can probably imagine, the number of weights for the self-attention layer is often in the tens of thousands, if not more. LoRA often allows fine-tuning on limited-hardware where a full fine-tuning pipeline may be infeasible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
