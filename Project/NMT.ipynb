{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0781a55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, itertools\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddf5216",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac49fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_len = []\n",
    "sql_len = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in df_train['question']:\n",
    "      ques_len.append(len(i.split()))  \n",
    "\n",
    "for i in df_train['sql']:\n",
    "      sql_len.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'question':ques_len, 'sql':sql_len})\n",
    "\n",
    "length_df.hist(bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f9eb946",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    normalized = unicodedata.normalize('NFD', s)\n",
    "    return ''.join(c for c in normalized if unicodedata.category(c) != 'Mn')\n",
    "     \n",
    "def preprocess_text(text):\n",
    "    text = unicode_to_ascii(text.lower().strip())\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n",
    "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    text = text.rstrip().strip()\n",
    "    text = '<sos> ' + text + ' <eos>'\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdc62deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_train.sql\n",
    "y = df_train.question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbf9f79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: SELECT Scoring average FROM table WHERE Money list rank = n/a\n",
      "Preprocessed sentence: <sos> select scoring average from table where money list rank n a <eos>\n"
     ]
    }
   ],
   "source": [
    "print('Original sentence:',x[42])\n",
    "x = [preprocess_text(w) for w in x]\n",
    "y = [preprocess_text(w) for w in y]\n",
    "print('Preprocessed sentence:',x[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cdbcaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame(x,columns=['question'])\n",
    "y = pd.DataFrame(y,columns=['sql'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "512f2232",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.concat([x,y],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff39ddb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>sql</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;sos&gt; select notes from table where current sl...</td>\n",
       "      <td>&lt;sos&gt; tell me what the notes are for south aus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;sos&gt; select current series from table where n...</td>\n",
       "      <td>&lt;sos&gt; what is the current series where the new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;sos&gt; select format from table where state ter...</td>\n",
       "      <td>&lt;sos&gt; what is the format for south australia ?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;sos&gt; select text background colour from table...</td>\n",
       "      <td>&lt;sos&gt; name the background colour for the austr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;sos&gt; select count fleet series quantity from ...</td>\n",
       "      <td>&lt;sos&gt; how many times is the fuel propulsion is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  <sos> select notes from table where current sl...   \n",
       "1  <sos> select current series from table where n...   \n",
       "2  <sos> select format from table where state ter...   \n",
       "3  <sos> select text background colour from table...   \n",
       "4  <sos> select count fleet series quantity from ...   \n",
       "\n",
       "                                                 sql  \n",
       "0  <sos> tell me what the notes are for south aus...  \n",
       "1  <sos> what is the current series where the new...  \n",
       "2  <sos> what is the format for south australia ?...  \n",
       "3  <sos> name the background colour for the austr...  \n",
       "4  <sos> how many times is the fuel propulsion is...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "463dc82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences): \n",
    "    lang_tokenizer = Tokenizer( filters='')\n",
    "    lang_tokenizer.fit_on_texts(sentences)\n",
    "    sequences = lang_tokenizer.texts_to_sequences(sentences)\n",
    "    max_length = max(len(s) for s in sequences)\n",
    "    sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "    return sequences, lang_tokenizer, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ad9efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_sequences(df, size=None):\n",
    "    x = df.question\n",
    "    y = df.sql\n",
    "\n",
    "    x,src_lang_tokenizer,max_length_src = tokenize(x)\n",
    "    y,tgt_lang_tokenizer,max_length_trg = tokenize(y)\n",
    "\n",
    "    return x, y, src_lang_tokenizer, tgt_lang_tokenizer, max_length_src, max_length_trg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fee64e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src sequences: (56355, 62)\n",
      "tgt sequences: (56355, 50)\n",
      "source maxlen: 62\n",
      "target maxlen: 50\n"
     ]
    }
   ],
   "source": [
    "x, y, src_lang_tokenizer, tgt_lang_tokenizer, max_length_src, max_length_trg = load_sequences(df_new)\n",
    "print(\"src sequences:\",x.shape)\n",
    "print(\"tgt sequences:\",y.shape)\n",
    "print(\"source maxlen:\",max_length_src)\n",
    "print(\"target maxlen:\",max_length_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f16544ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: When the money list rank was n/a, what was the scoring average?\n",
      "Text after preprocessing: <sos> when the money list rank was n a , what was the scoring average ? <eos>\n",
      "Text after tokenization : [   4    3 1860  109    1    2    6  178  472   24  166   50    5    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original sentence:\", df_train.question[42])\n",
    "print(\"Text after preprocessing:\", preprocess_text(df_train.question[42]))\n",
    "print(\"Text after tokenization :\", x[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91600faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62a4d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# def get_bert_embed(data):\n",
    "#     embedding = tokenizer.batch_encode_plus(data,\n",
    "#                 padding=True,              # Pad to the maximum sequence length\n",
    "#                 truncation=True,           # Truncate to the maximum sequence length if necessary\n",
    "#                 # return_tensors='pt',      # Return PyTorch tensors\n",
    "#                 add_special_tokens=True    # Add special tokens CLS and SEP\n",
    "#                 )\n",
    "#     return embedding\n",
    "\n",
    "# df_new.loc[:,'Encoded_Question_input_ids'] = pd.Series(get_bert_embed(df_new['question'])['input_ids'])\n",
    "# df_new.loc[:,'Encoded_query_input_ids'] = pd.Series(get_bert_embed(df_new['sql'])['input_ids'])\n",
    "\n",
    "\n",
    "# training_tensor_X = torch.tensor(df_new.Encoded_Question_input_ids)\n",
    "# training_tensor_y = torch.tensor(df_new.Encoded_query_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30638ec6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training_tensor_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ca76d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new.question[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "336b1fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_df = df_new.iloc[:,2:]\n",
    "# # encoded_df['Encoded_query_input_ids'][0]\n",
    "# encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89c65426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(encoded_df.Encoded_Question_input_ids[56354], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec1c998f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "4 ----> <sos>\n",
      "3 ----> select\n",
      "1860 ----> scoring\n",
      "109 ----> average\n",
      "1 ----> from\n",
      "2 ----> table\n",
      "6 ----> where\n",
      "178 ----> money\n",
      "472 ----> list\n",
      "24 ----> rank\n",
      "166 ----> n\n",
      "50 ----> a\n",
      "5 ----> <eos>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "2 ----> <sos>\n",
      "13 ----> when\n",
      "1 ----> the\n",
      "264 ----> money\n",
      "276 ----> list\n",
      "51 ----> rank\n",
      "10 ----> was\n",
      "218 ----> n\n",
      "9 ----> a\n",
      "8 ----> ,\n",
      "5 ----> what\n",
      "10 ----> was\n",
      "1 ----> the\n",
      "1026 ----> scoring\n",
      "47 ----> average\n",
      "4 ----> ?\n",
      "3 ----> <eos>\n"
     ]
    }
   ],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "\n",
    "print (\"Input Language; index to word mapping\")\n",
    "convert(src_lang_tokenizer, x[42])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(tgt_lang_tokenizer, y[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0b2db95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23971\n",
      "25515\n"
     ]
    }
   ],
   "source": [
    "\n",
    "src_vocab_size = len(src_lang_tokenizer.word_index)+1 \n",
    "tgt_vocab_size = len(tgt_lang_tokenizer.word_index)+1 \n",
    "print(src_vocab_size)\n",
    "print(tgt_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9beba0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train)=45084,len(X_test)=11271,len(y_train)=45084,len(y_test)=11271\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(x, y, shuffle=False, test_size=0.2)\n",
    "print(f\"{len(X_train)=},{len(X_test)=},{len(y_train)=},{len(y_test)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a4977da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    4     3   192     1     2     6   305 11119   142   339     5     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]]\n",
      "\n",
      "[[    4     3    15  7383  7384     1     2     6   864    94     7    18\n",
      "  21209 21210     5     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]]\n",
      "\n",
      "[[  2  91  87   5   1 288  43  12 180 401   3   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
      "\n",
      "[[    2    19  7765  7766    16     9  1008   131    55    14     8    11\n",
      "      9    40     7 22624 22625     4     3     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:1])\n",
    "print()\n",
    "print(X_test[:1])\n",
    "print()\n",
    "print(y_train[:1])\n",
    "print()\n",
    "print(y_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df66dc99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6788716a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab56a17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining hyperparameters\n",
    "buffer_size=len(X_train)\n",
    "val_buffer_size = len(X_test)\n",
    "BATCH_SIZE = 64\n",
    "embedding_dim = 128\n",
    "units = 1024 \n",
    "steps_per_epoch = buffer_size//BATCH_SIZE\n",
    "val_steps_per_epoch = val_buffer_size//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28f5d35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f970126",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=buffer_size).batch(BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2c16406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 62]), TensorShape([64, 50]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c33ecf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units \n",
    "        self.batch_sz = batch_sz \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, emb_dim,mask_zero=True)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform') \n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state \n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae9461af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 62, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(src_vocab_size, embedding_dim, units, BATCH_SIZE) \n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfec3205",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units) # fully-connected dense layer-1\n",
    "    self.W2 = tf.keras.layers.Dense(units) # fully-connected dense layer-2\n",
    "    self.V = tf.keras.layers.Dense(1) # fully-connected dense layer-3\n",
    "\n",
    "  def call(self, query, values):\n",
    "   \n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "    score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))   \n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc04b7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape (context vector): (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 62, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(20) \n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output) \n",
    "\n",
    "print(\"Attention result shape (context vector): (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f767acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, emb_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz \n",
    "    self.dec_units = dec_units \n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, emb_dim) \n",
    "    \n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform') \n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "    x = self.embedding(x)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "    output, state = self.gru(x)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    x = self.fc(output)\n",
    "    return x, state , attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57a24eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 25515)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(tgt_vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "729a9657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')  \n",
    "\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))  \n",
    "  loss_ = loss_object(real, pred)  \n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype) \n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'  \n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")  \n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,  \n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f7e8d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden) \n",
    "\n",
    "    dec_hidden = enc_hidden \n",
    "\n",
    "    dec_input = tf.expand_dims([tgt_lang_tokenizer.word_index['<sos>']] * BATCH_SIZE, 1) \n",
    "\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output) \n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions) \n",
    "\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1) \n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1])) \n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables \n",
    "\n",
    "  gradients = tape.gradient(loss, variables) \n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "164826bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def val_step(inp, targ, enc_hidden):\n",
    "    loss = 0 \n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden) \n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input =  tf.expand_dims([tgt_lang_tokenizer.word_index['<sos>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    for t in range(1, targ.shape[1]): \n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output) \n",
    "        loss += loss_function(targ[:, t], predictions) \n",
    "        dec_input = tf.expand_dims(targ[:, t], 1) \n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1])) \n",
    "\n",
    "    return batch_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df412127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_and_validate(train_dataset, val_dataset, EPOCHS=10):\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "\n",
    "        #Step1: \n",
    "        enc_hidden = encoder.initialize_hidden_state()\n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "        for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "            batch_loss = train_step(inp, targ, enc_hidden)\n",
    "            total_train_loss += batch_loss \n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                            batch,\n",
    "                                                            batch_loss.numpy()))\n",
    "       \n",
    "        for (batch, (inp, targ)) in enumerate(val_dataset.take(val_steps_per_epoch)):    \n",
    "            val_batch_loss = val_step(inp, targ, enc_hidden) \n",
    "            total_val_loss += val_batch_loss \n",
    "\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "        print('Total training loss is {:.4f}'.format(total_train_loss / steps_per_epoch))\n",
    "        print('Total validation loss is {:.4f}'.format( total_val_loss / val_steps_per_epoch))\n",
    "        print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f4ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate(train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "798be744",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_dataset\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')  \n",
    "\n",
    "checkpoint_dir = './training_checkpoints'  \n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")  \n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden, encoder, decoder):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden) \n",
    "        dec_hidden = enc_hidden \n",
    "        dec_input = tf.expand_dims([tgt_lang_tokenizer.word_index['<sos>']] * BATCH_SIZE, 1) \n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output) \n",
    "            loss += loss_function(targ[:, t], predictions) \n",
    "            dec_input = tf.expand_dims(targ[:, t], 1) \n",
    "    batch_loss = (loss / int(targ.shape[1])) \n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables \n",
    "    gradients = tape.gradient(loss, variables) \n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss\n",
    "\n",
    "@tf.function\n",
    "def val_step(inp, targ, enc_hidden, encoder, decoder):\n",
    "    loss = 0 \n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden) \n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input =  tf.expand_dims([tgt_lang_tokenizer.word_index['<sos>']] * BATCH_SIZE, 1)\n",
    "    for t in range(1, targ.shape[1]): \n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output) \n",
    "        loss += loss_function(targ[:, t], predictions) \n",
    "        dec_input = tf.expand_dims(targ[:, t], 1) \n",
    "    batch_loss = (loss / int(targ.shape[1])) \n",
    "    return batch_loss \n",
    "\n",
    "import time\n",
    "\n",
    "def train_and_validate(train_dataset, val_dataset, EPOCHS=10):\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "\n",
    "        enc_hidden = encoder.initialize_hidden_state()\n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "\n",
    "        for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "            batch_loss = train_step(inp, targ, enc_hidden, encoder, decoder)\n",
    "            total_train_loss += batch_loss \n",
    "            if batch % 100 == 0:\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
    "\n",
    "        for (batch, (inp, targ)) in enumerate(val_dataset.take(val_steps_per_epoch)):    \n",
    "            val_batch_loss = val_step(inp, targ, enc_hidden, encoder, decoder) \n",
    "            total_val_loss += val_batch_loss \n",
    "\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print('Total training loss is {:.4f}'.format(total_train_loss / steps_per_epoch))\n",
    "        print('Total validation loss is {:.4f}'.format(total_val_loss / val_steps_per_epoch))\n",
    "        print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6817d67c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
