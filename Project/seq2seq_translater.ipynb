{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq: Translation Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoderアーキテクチャ\n",
    "1. Sentence Data を3つのNumpy配列、（encoder_input_data、decoder_input_data、decoder_target_data）に変換.\n",
    "  - encoder_input_dataは、（num_pairs、max_english_sentence_length、num_english_characters）の3D配列. *英語文はOne-Hotベクトル\n",
    "  - decoder_input_dataは、（num_pairs、max_french_sentence_length、num_french_characters）の3D配列. *日本語文はOne-Hotベクトル\n",
    "  - decoder_target_dataは、decoder_input_dataと同じだが、1 Time Step オフセットされている.\n",
    "2. encoder_input_dataとdecoder_input_dataが与えられ、decoder_target_dataを予測するために基本的なLSTMベースのSeq2Seqモデルを訓練する.\n",
    "3. いくつかのサンプル文をデコードして、モデルが機能していることを確認する."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn-images-1.medium.com/max/2600/1*1I2tTjCkMHlQ-r73eRn4ZQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import pickle\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocess feeding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"jpn.txt\", 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43954"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "bos = \"<BOS> \"\n",
    "eos = \" <EOS>\"\n",
    "\n",
    "for line in lines:\n",
    "    input_text, target_text = line.split(\"\\t\")\n",
    "    target_text = bos + target_text + eos\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Five tremors in excess of magnitude 5.0 on the Richter scale have shaken Japan just this week, but scientists are warning that the largest expected aftershock has yet to hit.',\n",
       " 'The bus now arriving is going to Domestic Terminal 1. Passengers for the International Terminal, please wait. The shuttle bus to the International Terminal also leaves from this stop.',\n",
       " 'A child who is a native speaker usually knows many things about his or her language that a non-native speaker who has been studying for years still does not know and perhaps will never know.',\n",
       " \"I do many things at the same time, so not only am I reading things by Akutagawa, I've also increased the amount of time I spend reading in English and I also read a little in German every day.\"]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\t日本ではリヒター・スケールでマグニチュード5.0以上の余震が今週5回ありました。しかし科学者たちは、最大規模の余震はまだ先に起こりうると警告しています。\\n',\n",
       " '\\tただ今到着のバスは、国内線第1ターミナル行きです。国際線ターミナルにお越しの方は、しばらくそのままでお待ちください。国際線行き連絡バスもこのバス停から発車いたします。\\n',\n",
       " '\\tネイティブの子どもは、何年も学んだ非ネイティブが知らず今後も知り得ないたくさんのことを自身の言語について知っているものだ。\\n',\n",
       " '\\t色々並行してやってるから芥川ばかり読んでるのでもないのだよ。今は英語読んでる時間が増えてる。ドイツ語も毎日少しずつやってる。\\n']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 英語をcharacter単位で分解\n",
    "input_characters = set()\n",
    "\n",
    "for text in input_texts:\n",
    "    char = text.split()\n",
    "    for char in input_texts:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "english_char_size = len(input_characters)\n",
    "english_char_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語をcharacter単位で分解\n",
    "target_characters = set()\n",
    "\n",
    "for text in target_texts:\n",
    "    char = text.split()\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "target_characters = sorted(list(target_characters))\n",
    "japanese_char_size = len(target_characters)\n",
    "japanese_char_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最大文章の長さをえる\n",
    "english_maxlen = max([ len(text) for text in input_texts ])\n",
    "japanese_maxlen = max([ len(text) for text in target_texts_texts ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辞書づくり"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_char_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_char_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_index_char = dict(\n",
    "    (i, char) for char, i in input_char_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_index_char = dict(\n",
    "    (i, char) for char, i in target_char_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Dテンソル (サンプル数, MAX_LEN, 辞書サイズ)\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), english_maxlen, english_char_size),\n",
    "    dtype=\"float32\")\n",
    "\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), japanese_maxlen, japanese_char_size),\n",
    "    dtype=\"float32\")\n",
    "\n",
    "decoder_output_data = np.zeros(\n",
    "    (len(input_texts), japanese_maxlen, japanese_char_size),\n",
    "    dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(input_texts):\n",
    "    for j, char in enumerate(text):\n",
    "        encoder_input_data[i][j][input_char_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(target_texts):\n",
    "    for j, char in enumerate(text):\n",
    "        decoder_input_data[i][j][target_char_index[char]] = 1.\n",
    "        # decoderのアウトプットは1ステップずれる\n",
    "        if j > 0:\n",
    "            decoder_output_data[i, j-1, target_char_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process\n",
    "1. Encode the input sequence into state vectors.\n",
    "2. Start with a target sequence of size 1 (just the start-of-sequence character).\n",
    "3. Feed the state vectors and 1-char target sequence to the decoder to produce predictions for the next character.\n",
    "4. Sample the next character using these predictions (we simply use argmax).\n",
    "5. Append the sampled character to the target sequence\n",
    "6. Repeat until we generate the end-of-sequence character or we hit the character limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encoder入力データ: id化された単語シーケンス（英語）\n",
    "- Encoder入力層: None×単語シーケンスの長さの2次元マトリックス\n",
    "- EncoderのLSTM層: 指定の隠れ層のユニット数\n",
    "- Decoder入力データ: id化された単語シーケンス（日本語）\n",
    "- Decoder入力層: None×単語シーケンスの長さの2次元マトリックス\n",
    "- DecoderのLSTM層: 指定の隠れ層のユニット数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 256\n",
    "NUM_ENCODER_TOKENS = english_char_size\n",
    "NUM_DECODER_TOKENS = japanese_char_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, NUM_ENCODER_TOKENS))\n",
    "encoder_LSTM = LSTM(units=HIDDEN_DIM, return_state=True) # this is the key\n",
    "encoder_outputs, state_h, state_c = encoder_LSTM(encoder_inputs) # encoderの出力は無視する、memory cellの状態だけ保存\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, NUM_DECODER_TOKENS))\n",
    "decoder_LSTM = LSTM(units=HIDDEN_DIM, return_sequences=True, return_state=True) # this is the key\n",
    "decoder_outputs, _h, _c = decoder_LSTM(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(units=NUM_DECODER_TOKENS, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 30)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 30)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 64), (None,  24320       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 64), ( 24320       input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 30)     1950        lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 50,590\n",
      "Trainable params: 50,590\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file = \"seq2seq_translation.png\", show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Layer works as encoder and decoder\n",
    "- Encoder: 入力シーケンスを処理し、それ自体の内部状態を返す.この内部状態は、次のステップでDecoderの「文脈」と「状態」として機能する.\n",
    "- Decoder: 1つ前の文字があれば、ターゲットシーケンスの次の文字を予測するように訓練されている. \n",
    "- Thought Vector: Encoderは初期状態としてEncoderからの状態ベクトルを使用. これはDecoderが生成するものに関する情報を取得するメソッド."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- return_stateコンストラクタ引数。\n",
    "- 最初のエントリが出力で次のエントリが内部RNN状態であるリストを返すようにRNNレイヤを設定します。\n",
    "- これはエンコーダの状態を回復するために使用されます。\n",
    "- RNNの初期状態を指定するinital_state呼び出し引数。\n",
    "- これは、初期状態としてエンコーダ状態をデコーダに渡すために使用されます。\n",
    "- return_sequencesコンストラクター引数。\n",
    "- RNNがその完全な出力シーケンスを返すように設定します（デフォルトの動作である最後の出力だけではなく）。\n",
    "- これはデコーダで使用されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fitting and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_output_data,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_split=0.2)\n",
    "\n",
    "model.save(\"seq2seq_translation.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "BATCH_SIZE:  32\n",
      "EPOCHS:  5\n",
      "Train on 35162 samples, validate on 8791 samples\n",
      "Epoch 1/5\n",
      "22048/35162 [=================>............] - ETA: 2:04 - loss: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = [32, 64]\n",
    "EPOCHS = [5, 10]\n",
    "\n",
    "for i in range(2):\n",
    "    BATCH_SIZE = BATCH_SIZE[i]\n",
    "    for e in range(2):\n",
    "        EPOCHS = EPOCHS[e]\n",
    "        print(\"------------------------------------\")\n",
    "        print(\"BATCH_SIZE: \", BATCH_SIZE)\n",
    "        print(\"EPOCHS: \", EPOCHS)\n",
    "        history = model.fit([encoder_input_data, decoder_input_data], decoder_output_data,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            epochs=EPOCHS,\n",
    "                            validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(HIDDEN_DIM,))\n",
    "decoder_state_input_c = Input(shape=(HIDDEN_DIM,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, _h, _c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [_h, _c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_translater(english_input):\n",
    "    \n",
    "    encoder_states = encoder_model.predict(english_input)\n",
    "\n",
    "    japanese_seq = np.zeros((1, 1, NUM_DECODER_TOKENS))\n",
    "    japanese_seq[0, 0, target_char_index[bos]] = 1.\n",
    "    \n",
    "    stop_condition = False\n",
    "    japanese_output = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([japanese_seq] + encoder_states)\n",
    "        # ソフトマックスが最大のcharのidを返す np.argmax\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        # 日本語の単語に変換\n",
    "        sampled_char = target_index_char[sampled_token_index]\n",
    "        japanese_output += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length or find stop character.\n",
    "        if (sampled_char == eos or len(japanese_output) > japanese_maxlen):\n",
    "            stop_condition = True\n",
    "\n",
    "        japanese_seq = np.zeros((1, 1, NUM_DECODER_TOKENS))\n",
    "        japanese_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        encoder_states = [_h, _c]\n",
    "\n",
    "    return japanese_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_text = \"I am so tired and I feel bored to go back to my home.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_translater(english_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
