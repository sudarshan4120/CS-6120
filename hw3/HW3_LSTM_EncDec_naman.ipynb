{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c3ab45d-c79f-465d-8041-78bf2a58173d",
   "metadata": {
    "id": "7c3ab45d-c79f-465d-8041-78bf2a58173d"
   },
   "source": [
    "Task 4: Train an LSTM Model (40 points)\n",
    "----\n",
    "1. Using PyTorch, implement a neural network that uses one or more LSTM cells to do sentiment analysis. Use the nn.Embedding, nn.LSTM and nn.Linear layers to construct your model.\n",
    "2. Note that sequence processing works differently with the PyTorch Embedding layer as compared to my sample code from class. The model input expects a padded tensor of token indices from the vocabulary, instead of one-hot encodings. For evaluation, use a vocabulary size of 10000 (max_features = 10000).\n",
    "3. The model should have a single output with the sigmoid activation function for classification. The dimensions of the embedding layer and the hidden layer(s) are up to you, but please make sure your model does not take more than ~3 minutes to train.\n",
    "4. Evaluate the model using PyTorch functions for average accuracy, area under the ROC curve and F1 scores (see [torchedev](https://pytorch.org/torcheval/stable/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04554ea0-360d-43cd-b5d4-67442da6dbb6",
   "metadata": {
    "id": "04554ea0-360d-43cd-b5d4-67442da6dbb6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d558af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics.functional import binary_f1_score\n",
    "from torcheval.metrics import BinaryAUROC, BinaryAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b01e664a-e80a-4129-968c-3a3df25617a5",
   "metadata": {
    "id": "b01e664a-e80a-4129-968c-3a3df25617a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d028c44-0bcc-47a1-b9fe-78d1ddc030f2",
   "metadata": {
    "id": "2d028c44-0bcc-47a1-b9fe-78d1ddc030f2"
   },
   "outputs": [],
   "source": [
    "train_data_file = 'movie_reviews_train.txt'\n",
    "train_df = pd.read_csv(train_data_file, sep='\\t', header=None, names=['id', 'review', 'label'])[['review', 'label']]\n",
    "X_train, y_train = train_df['review'].values, train_df['label'].values\n",
    "\n",
    "dev_data_file = 'movie_reviews_dev.txt'\n",
    "dev_df = pd.read_csv(dev_data_file, sep='\\t', header=None, names=['id', 'review', 'label'])[['review', 'label']]\n",
    "X_dev, y_dev = dev_df['review'].values, dev_df['label'].values\n",
    "\n",
    "test_data_file = 'movie_reviews_test.txt'\n",
    "test_df = pd.read_csv(test_data_file, sep='\\t', header=None, names=['id', 'review', 'label'])[['review', 'label']]\n",
    "X_test, y_test = test_df['review'].values, test_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa0d2b64-25f0-4bdd-a6ff-8a8aea1c586b",
   "metadata": {
    "id": "fa0d2b64-25f0-4bdd-a6ff-8a8aea1c586b"
   },
   "outputs": [],
   "source": [
    "def preprocess_token(s): \n",
    "    \"\"\"\n",
    "      This function is for pre-processing each token, not the entire sequence\n",
    "      Retain only alphanumeric characters\n",
    "      replace digits with no space\n",
    "      Replace all whitespace sequences with no space\n",
    "    \"\"\"\n",
    "    \n",
    "    s = re.sub(\"[^\\w]\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def clean_words(sentence: list[str], del_words: list[str], lowercase: bool = False):\n",
    "    if lowercase:\n",
    "        cleaned = list(map(lambda x: x.lower(), sentence))\n",
    "    cleaned = list(filter(lambda word: word not in del_words, cleaned))\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def max_length_in_list_of_lists(list_of_lists):\n",
    "    return max(len(sublist) for sublist in list_of_lists)\n",
    "\n",
    "def encode(corpus: list[list[str]], vocab_map: dict[int]):\n",
    "    processed_corp = []\n",
    "    \n",
    "    for document in corpus:\n",
    "        enc_doc = []\n",
    "        for token in document:\n",
    "            if token in vocab_map.keys():\n",
    "                enc_doc.append(vocab_map[token])\n",
    "        \n",
    "        processed_corp.append(enc_doc)\n",
    "    \n",
    "    return processed_corp\n",
    "\n",
    "def pad(sentence:list, length:int):\n",
    "    sentence.extend([0]*abs(len(sentence)-length))\n",
    "    return sentence\n",
    "    \n",
    "def tokenize(x_train: list[str],\n",
    "             x_dev: list[str],\n",
    "             x_test: list[str],\n",
    "             vocab_size: int,\n",
    "             stopwords: list[str] = stopwords.words('english')):\n",
    "    \n",
    "    # spliting to tokens\n",
    "    x_train = [x.split() for x in x_train]\n",
    "    x_dev = [x.split() for x in x_dev]\n",
    "    x_test = [x.split() for x in x_test]\n",
    "\n",
    "    # Iterate through each document in the data and preprocess\n",
    "    processed_data = []\n",
    "    for doc in x_train:\n",
    "        temp = list(map(lambda x: preprocess_token(x), doc))\n",
    "        try:\n",
    "            temp.remove('')\n",
    "        except ValueError:\n",
    "            ...\n",
    "        processed_data.append(temp)\n",
    "    x_train = list(processed_data)\n",
    "    \n",
    "    processed_data = []\n",
    "    for doc in x_test:\n",
    "        temp = list(map(lambda x: preprocess_token(x), doc))\n",
    "        try:\n",
    "            temp.remove('')\n",
    "        except ValueError:\n",
    "            ...\n",
    "        processed_data.append(temp)\n",
    "    x_test = list(processed_data)\n",
    "    \n",
    "    processed_data = []\n",
    "    for doc in x_dev:\n",
    "        temp = list(map(lambda x: preprocess_token(x), doc))\n",
    "        try:\n",
    "            temp.remove('')\n",
    "        except ValueError:\n",
    "            ...\n",
    "        processed_data.append(temp)\n",
    "    x_dev = list(processed_data)\n",
    "    \n",
    "    # process stop words to match with vocab\n",
    "    stopwords = list(map(lambda word: preprocess_token(word), stopwords))\n",
    "    \n",
    "    # Remove stop words\n",
    "    x_train = list(map(lambda x: clean_words(x, stopwords, lowercase=True), x_train))\n",
    "    x_dev = list(map(lambda x: clean_words(x, stopwords, lowercase=True), x_dev))\n",
    "    x_test = list(map(lambda x: clean_words(x, stopwords, lowercase=True), x_test))          \n",
    "    \n",
    "    # Creating a unified token list\n",
    "    all_tok = []\n",
    "    for doc in x_train:\n",
    "        all_tok.extend(doc)\n",
    "    for doc in x_test:\n",
    "        all_tok.extend(doc)\n",
    "    for doc in x_dev:\n",
    "        all_tok.extend(doc)\n",
    "    all_toks = Counter(all_tok)\n",
    "    \n",
    "    # Retain the 'vocab_size' most frequent words\n",
    "    freq_vocab_map = {}\n",
    "    for index, freq in enumerate(all_toks.most_common(vocab_size)):\n",
    "        freq_vocab_map[freq[0]] = index + 1\n",
    "\n",
    "    # Initialize empty lists to store padded sequences for training, development, and testing data\n",
    "    padd_train = padd_test = padddev = []\n",
    "    \n",
    "    # convert tokens to their corresponding indices in the vocabulary if they exist\n",
    "    padd_train = encode(x_train, freq_vocab_map)\n",
    "    padd_test = encode(x_test, freq_vocab_map)\n",
    "    padd_dev = encode(x_dev, freq_vocab_map)\n",
    "    \n",
    "        \n",
    "    # Determine the maximum sequence size among all datasets (training, development, and testing)\n",
    "    max_len = max(max_length_in_list_of_lists(padd_train),\n",
    "                  max_length_in_list_of_lists(padd_test),\n",
    "                  max_length_in_list_of_lists(padd_dev))\n",
    "    \n",
    "    padd_train = np.array(list(map(lambda x: pad(x, max_len), padd_train)))\n",
    "    padd_test = np.array(list(map(lambda x: pad(x, max_len), padd_test)))\n",
    "    padd_dev = np.array(list(map(lambda x: pad(x, max_len), padd_dev)))\n",
    "    \n",
    "    return (padd_train, padd_dev, padd_test, freq_vocab_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a926d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize your train, test and development data\n",
    "X_train, X_dev, X_test, vocab = tokenize(X_train, X_dev, X_test, 100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee92ad20-ceea-4579-ae72-03b4cb0d7b39",
   "metadata": {
    "id": "ee92ad20-ceea-4579-ae72-03b4cb0d7b39"
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train.astype(np.float32)))\n",
    "dev_data = TensorDataset(torch.from_numpy(X_dev), torch.from_numpy(y_dev.astype(np.float32)))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test.astype(np.float32)))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 25\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "dev_loader = DataLoader(dev_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dea3832-6ad4-4c9c-845b-bfbaa04dc03a",
   "metadata": {
    "id": "0dea3832-6ad4-4c9c-845b-bfbaa04dc03a"
   },
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self,num_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n",
    "        super(SentimentRNN,self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout_layer = nn.Dropout(drop_prob)\n",
    "\n",
    "        # linear and sigmoid layer\n",
    "        self.linear_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x,hidden):\n",
    "\n",
    "        # embeddings\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
    "\n",
    "        # stack up LSTM outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout_layer(lstm_out)\n",
    "        out = self.linear_layer(out)\n",
    "\n",
    "        # sigmoid function\n",
    "        sig_out = self.activation(out)\n",
    "\n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "\n",
    "        # get the last batch of labels\n",
    "        sig_out = sig_out[:, -1]\n",
    "        \n",
    "        return sig_out, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # initialize hidden state(s) and cell state(s) of LSTM to zero with appropriate dimensions\n",
    "        h0 = torch.zeros((self.num_layers, batch_size, self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.num_layers, batch_size, self.hidden_dim)).to(device)\n",
    "        hidden = (h0,c0)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76fbf614-ddc3-475a-a488-a8dddad7282b",
   "metadata": {
    "id": "76fbf614-ddc3-475a-a488-a8dddad7282b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = int(32)\n",
    "output_dim = 1\n",
    "hidden_dim = 256\n",
    "\n",
    "model = SentimentRNN(num_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33ad0210",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(32202, 32)\n",
       "  (lstm): LSTM(32, 256, num_layers=4, batch_first=True, dropout=0.3)\n",
       "  (dropout_layer): Dropout(p=0.3, inplace=False)\n",
       "  (linear_layer): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8d96d5d-aa54-41e0-856f-03cd7a740029",
   "metadata": {
    "id": "b8d96d5d-aa54-41e0-856f-03cd7a740029"
   },
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "\n",
    "# you should use binary cross-entropy as your loss function and Adam optimizer for this task\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "loss_func = nn.BCELoss()\n",
    "\n",
    "# function to predict accuracy\n",
    "def acc(preds, labels, threshold=0.5):\n",
    "    binary_predictions = (preds > threshold).float()\n",
    "    return torch.tensor(torch.sum(binary_predictions == labels).item() / len(preds))\n",
    "\n",
    "def mean(listt):\n",
    "    return sum(listt)/len(listt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b502a0ea-6570-47a6-a7b8-9d0a307d0541",
   "metadata": {
    "id": "b502a0ea-6570-47a6-a7b8-9d0a307d0541",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "train_loss : 0.6936050280928612 dev_loss : 0.6949827894568443\n",
      "train_accuracy : 0.48875001072883606 dev_accuracy : 0.4650000333786011\n",
      "==================================================\n",
      "Epoch 2\n",
      "train_loss : 0.6941576525568962 dev_loss : 0.6935289204120636\n",
      "train_accuracy : 0.49187496304512024 dev_accuracy : 0.5200000405311584\n",
      "==================================================\n",
      "Epoch 4\n",
      "train_loss : 0.6932710977271199 dev_loss : 0.6941909193992615\n",
      "train_accuracy : 0.5037499666213989 dev_accuracy : 0.45499998331069946\n",
      "==================================================\n",
      "Epoch 6\n",
      "train_loss : 0.6944630108773708 dev_loss : 0.692916750907898\n",
      "train_accuracy : 0.4818749725818634 dev_accuracy : 0.5049999952316284\n",
      "==================================================\n",
      "Epoch 8\n",
      "train_loss : 0.6936004627496004 dev_loss : 0.6921800076961517\n",
      "train_accuracy : 0.4818749725818634 dev_accuracy : 0.5300000309944153\n",
      "==================================================\n",
      "Epoch 10\n",
      "train_loss : 0.6931778881698847 dev_loss : 0.6931089609861374\n",
      "train_accuracy : 0.5081250071525574 dev_accuracy : 0.49500006437301636\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "clip = 5\n",
    "epochs = 10\n",
    "dev_loss_min = np.Inf\n",
    "best_epoch = 0\n",
    "\n",
    "epoch_tr_loss,epoch_dv_loss = [],[]\n",
    "epoch_tr_acc,epoch_dv_acc = [],[]\n",
    "\n",
    "for epoch in range(epochs): # Train your model\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    dev_loss = []\n",
    "    dev_acc = []\n",
    "    \n",
    "    for features, target in train_loader:\n",
    "        features = features.to(device)\n",
    "        target = target.to(device)\n",
    "        hidden_state = model.init_hidden(batch_size)\n",
    "        out, _ = model(features, hidden_state)\n",
    "        loss = loss_func(out, target)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.append(loss.item())\n",
    "        train_acc.append(acc(out, target))\n",
    "    \n",
    "    for features, target in dev_loader:\n",
    "        features = features.to(device)\n",
    "        target = target.to(device)\n",
    "        hidden_state = model.init_hidden(batch_size)\n",
    "        out, _ = model(features, hidden_state)\n",
    "        loss = loss_func(out, target)\n",
    "        dev_loss.append(loss.item())\n",
    "        dev_acc.append(acc(out, target))\n",
    "    \n",
    "    mean_dev_loss = mean(dev_loss)\n",
    "    mean_train_loss = mean(train_loss)\n",
    "    mean_train_acc = mean(train_acc)\n",
    "    mean_dev_acc = mean(dev_acc)\n",
    "    \n",
    "    if (epoch+1)%2==0 or epoch==0:\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        print(f'train_loss : {mean_train_loss} dev_loss : {mean_dev_loss}')\n",
    "        print(f'train_accuracy : {mean_train_acc} dev_accuracy : {mean_dev_acc}')\n",
    "        print(25*'==')\n",
    "\n",
    "    # if dev_loss goes less than or equal to dev_loss_min then save your model and update the dev_loss_min\n",
    "\n",
    "    if mean_dev_loss<dev_loss_min:\n",
    "        dev_loss_min = mean_dev_loss\n",
    "        # save model here\n",
    "        torch.save(model.state_dict(), f'best_model.pth')\n",
    "        best_epoch = epoch+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "875caa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch Model = 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Epoch Model = {best_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa883eea",
   "metadata": {},
   "source": [
    "NOTE: your train loss should be smaller than 1 and your train accuracy should be over 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ade8442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "test_acc = 0.0\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Iterate over test data batches\n",
    "for inputs, labels in test_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    test_h = model.init_hidden(batch_size)\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs, _ = model(inputs, test_h)\n",
    "    \n",
    "    # Compute predictions\n",
    "    predictions.extend(outputs.cpu().numpy())  # Assuming outputs are on CPU\n",
    "    true_labels.extend(labels.cpu().numpy())  # Assuming labels are on CPU\n",
    "\n",
    "# Convert predictions and true_labels to numpy arrays\n",
    "predictions = torch.tensor(predictions)\n",
    "true_labels = torch.tensor(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43d4d252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score - 0.6666666865348816\n",
      "Area under ROC 0.4693013408609739\n",
      "Accuracy 0.4693013408609739\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1 Score - {binary_f1_score(predictions, true_labels)}\")\n",
    "metric = BinaryAUROC()\n",
    "metric.update(predictions, true_labels)\n",
    "print(f\"Area under ROC {metric.compute()}\")\n",
    "acc_metric = BinaryAccuracy()\n",
    "acc_metric.update(predictions, true_labels)\n",
    "print(f\"Accuracy {metric.compute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f332ea53",
   "metadata": {},
   "source": [
    "NOTE: your eval accuracy should be of at least 60%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8917a512",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch_temp",
   "language": "python",
   "name": "torch_temp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
