{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c3ab45d-c79f-465d-8041-78bf2a58173d",
   "metadata": {
    "id": "7c3ab45d-c79f-465d-8041-78bf2a58173d"
   },
   "source": [
    "Task 4: Train an LSTM Model (40 points)\n",
    "----\n",
    "1. Using PyTorch, implement a neural network that uses one or more LSTM cells to do sentiment analysis. Use the nn.Embedding, nn.LSTM and nn.Linear layers to construct your model.\n",
    "2. Note that sequence processing works differently with the PyTorch Embedding layer as compared to my sample code from class. The model input expects a padded tensor of token indices from the vocabulary, instead of one-hot encodings. For evaluation, use a vocabulary size of 10000 (max_features = 10000).\n",
    "3. The model should have a single output with the sigmoid activation function for classification. The dimensions of the embedding layer and the hidden layer(s) are up to you, but please make sure your model does not take more than ~3 minutes to train.\n",
    "4. Evaluate the model using PyTorch functions for average accuracy, area under the ROC curve and F1 scores (see [torchedev](https://pytorch.org/torcheval/stable/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04554ea0-360d-43cd-b5d4-67442da6dbb6",
   "metadata": {
    "id": "04554ea0-360d-43cd-b5d4-67442da6dbb6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b01e664a-e80a-4129-968c-3a3df25617a5",
   "metadata": {
    "id": "b01e664a-e80a-4129-968c-3a3df25617a5"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d028c44-0bcc-47a1-b9fe-78d1ddc030f2",
   "metadata": {
    "id": "2d028c44-0bcc-47a1-b9fe-78d1ddc030f2"
   },
   "outputs": [],
   "source": [
    "train_data_file = 'movie_reviews_train.txt'\n",
    "train_df = pd.read_csv(train_data_file, sep='\\t', header=None, names=['id', 'review', 'label'])[['review', 'label']]\n",
    "X_train, y_train = train_df['review'].values, train_df['label'].values\n",
    "\n",
    "dev_data_file = 'movie_reviews_dev.txt'\n",
    "dev_df = pd.read_csv(dev_data_file, sep='\\t', header=None, names=['id', 'review', 'label'])[['review', 'label']]\n",
    "X_dev, y_dev = dev_df['review'].values, dev_df['label'].values\n",
    "\n",
    "test_data_file = 'movie_reviews_test.txt'\n",
    "test_df = pd.read_csv(test_data_file, sep='\\t', header=None, names=['id', 'review', 'label'])[['review', 'label']]\n",
    "X_test, y_test = test_df['review'].values, test_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0d2b64-25f0-4bdd-a6ff-8a8aea1c586b",
   "metadata": {
    "id": "fa0d2b64-25f0-4bdd-a6ff-8a8aea1c586b"
   },
   "outputs": [],
   "source": [
    "def preprocess_token(s): # This function is for pre-processing each token, not the entire sequence\n",
    "    # Retain only alphanumeric characters\n",
    "\n",
    "    # replace digits with no space\n",
    "\n",
    "    # Replace all whitespace sequences with no space\n",
    "\n",
    "    return s\n",
    "\n",
    "def tokenize(x_train, x_dev, x_test, vocab_size): # This function is for pre-processing strings, which uses the above.\n",
    "\n",
    "\n",
    "    # Remove stop words\n",
    "\n",
    "    # Retain the 'vocab_size' most frequent words\n",
    "\n",
    "\n",
    "    # Initialize empty lists to store padded sequences for training, development, and testing data\n",
    "\n",
    "\n",
    "    # Iterate through each document in the training data\n",
    "\n",
    "    for doc in x_train:\n",
    "\n",
    "        # Tokenize the document, convert tokens to lowercase, and preprocess each token\n",
    "        # Then, convert tokens to their corresponding indices in the vocabulary if they exist\n",
    "\n",
    "        pass\n",
    "\n",
    "    # Iterate through each document in the development data\n",
    "\n",
    "    for doc in x_dev:\n",
    "\n",
    "        # Tokenize the document, convert tokens to lowercase, and preprocess each token\n",
    "        # Then, convert tokens to their corresponding indices in the vocabulary if they exist\n",
    "\n",
    "        pass\n",
    "\n",
    "    # Iterate through each document in the testing data\n",
    "\n",
    "    for doc in x_test:\n",
    "\n",
    "        # Tokenize the document, convert tokens to lowercase, and preprocess each token\n",
    "        # Then, convert tokens to their corresponding indices in the vocabulary if they exist\n",
    "\n",
    "        pass\n",
    "\n",
    "    # Determine the maximum sequence size among all datasets (training, development, and testing)\n",
    "\n",
    "\n",
    "    # Pad sequences in the training, testing and development data to ensure uniform length using zero-padding\n",
    "\n",
    "    # Finally, return the padded sequences (train, development and test) and vocabulary\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912fd19f-04b5-4549-b944-142b99be21f7",
   "metadata": {
    "id": "912fd19f-04b5-4549-b944-142b99be21f7"
   },
   "outputs": [],
   "source": [
    "# Tokenize your train, test and development data\n",
    "\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee92ad20-ceea-4579-ae72-03b4cb0d7b39",
   "metadata": {
    "id": "ee92ad20-ceea-4579-ae72-03b4cb0d7b39"
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "dev_data = TensorDataset(torch.from_numpy(X_dev), torch.from_numpy(y_dev))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "dev_loader = DataLoader(dev_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dea3832-6ad4-4c9c-845b-bfbaa04dc03a",
   "metadata": {
    "id": "0dea3832-6ad4-4c9c-845b-bfbaa04dc03a"
   },
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self,num_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n",
    "        super(SentimentRNN,self).__init__()\n",
    "\n",
    "\n",
    "        # embedding and LSTM layers\n",
    "\n",
    "        ###### YOUR CODE HERE #######\n",
    "\n",
    "\n",
    "        # lstm\n",
    "\n",
    "        ###### YOUR CODE HERE #######\n",
    "\n",
    "\n",
    "        # dropout layer\n",
    "\n",
    "        ###### YOUR CODE HERE #######\n",
    "\n",
    "\n",
    "        # linear and sigmoid layer\n",
    "\n",
    "        ###### YOUR CODE HERE #######\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x,hidden):\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # initialize hidden state(s) and cell state(s) of LSTM to zero with appropriate dimensions\n",
    "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        hidden = (h0,c0)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76fbf614-ddc3-475a-a488-a8dddad7282b",
   "metadata": {
    "id": "76fbf614-ddc3-475a-a488-a8dddad7282b"
   },
   "outputs": [],
   "source": [
    "no_layers = 4\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 64\n",
    "output_dim = 1\n",
    "hidden_dim = 256\n",
    "\n",
    "model = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n",
    "\n",
    "#moving to gpu\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d96d5d-aa54-41e0-856f-03cd7a740029",
   "metadata": {
    "id": "b8d96d5d-aa54-41e0-856f-03cd7a740029"
   },
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "\n",
    "# you should use binary cross-entropy as your loss function and Adam optimizer for this task\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "# function to predict accuracy\n",
    "def acc(pred,label):\n",
    "\n",
    "    ### YOUR CODE HERE ###\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b502a0ea-6570-47a6-a7b8-9d0a307d0541",
   "metadata": {
    "id": "b502a0ea-6570-47a6-a7b8-9d0a307d0541"
   },
   "outputs": [],
   "source": [
    "clip = 5\n",
    "epochs = 5\n",
    "dev_loss_min = np.Inf\n",
    "\n",
    "epoch_tr_loss,epoch_dv_loss = [],[]\n",
    "epoch_tr_acc,epoch_dv_acc = [],[]\n",
    "\n",
    "for epoch in range(epochs): # Train your model\n",
    "\n",
    "    ### YOUR CODE HERE ###\n",
    "\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    print(f'train_loss : YOUR_TRAIN_LOSS_HERE dev_loss : YOUR_DEV_LOSS_HERE')\n",
    "    print(f'train_accuracy : YOUR_ACC_HERE dev_accuracy : YOUR_DEV_ACC_HERE')\n",
    "\n",
    "    # if dev_loss goes less than or equal to dev_loss_min then save your model and update the dev_loss_min\n",
    "\n",
    "    ### YOUR CODE HERE ###\n",
    "\n",
    "    print(25*'==')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa883eea",
   "metadata": {},
   "source": [
    "NOTE: your train loss should be smaller than 1 and your train accuracy should be over 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd036fbe-9b75-4217-add1-20dd33cc48a3",
   "metadata": {
    "id": "cd036fbe-9b75-4217-add1-20dd33cc48a3"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_h = model.init_hidden(batch_size)\n",
    "test_acc = 0.0\n",
    "\n",
    "# Evaluate model on your test data and report the accuracy\n",
    "\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f332ea53",
   "metadata": {},
   "source": [
    "NOTE: your eval accuracy should be of at least 60%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8917a512",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
