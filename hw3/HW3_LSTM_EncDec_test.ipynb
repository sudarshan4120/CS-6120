{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c3ab45d-c79f-465d-8041-78bf2a58173d",
   "metadata": {
    "id": "7c3ab45d-c79f-465d-8041-78bf2a58173d"
   },
   "source": [
    "Task 4: Train an LSTM Model (40 points)\n",
    "----\n",
    "1. Using PyTorch, implement a neural network that uses one or more LSTM cells to do sentiment analysis. Use the nn.Embedding, nn.LSTM and nn.Linear layers to construct your model.\n",
    "2. Note that sequence processing works differently with the PyTorch Embedding layer as compared to my sample code from class. The model input expects a padded tensor of token indices from the vocabulary, instead of one-hot encodings. For evaluation, use a vocabulary size of 10000 (max_features = 10000).\n",
    "3. The model should have a single output with the sigmoid activation function for classification. The dimensions of the embedding layer and the hidden layer(s) are up to you, but please make sure your model does not take more than ~3 minutes to train.\n",
    "4. Evaluate the model using PyTorch functions for average accuracy, area under the ROC curve and F1 scores (see [torchedev](https://pytorch.org/torcheval/stable/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04554ea0-360d-43cd-b5d4-67442da6dbb6",
   "metadata": {
    "id": "04554ea0-360d-43cd-b5d4-67442da6dbb6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b01e664a-e80a-4129-968c-3a3df25617a5",
   "metadata": {
    "id": "b01e664a-e80a-4129-968c-3a3df25617a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU used\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d028c44-0bcc-47a1-b9fe-78d1ddc030f2",
   "metadata": {
    "id": "2d028c44-0bcc-47a1-b9fe-78d1ddc030f2"
   },
   "outputs": [],
   "source": [
    "train_data_file = 'movie_reviews_train.txt'\n",
    "train_df = pd.read_csv(train_data_file, sep='\\t', header=None, names=['id', 'review', 'label'])[['review', 'label']]\n",
    "X_train, y_train = train_df['review'].values, train_df['label'].values\n",
    "\n",
    "dev_data_file = 'movie_reviews_dev.txt'\n",
    "dev_df = pd.read_csv(dev_data_file, sep='\\t', header=None, names=['id', 'review', 'label'])[['review', 'label']]\n",
    "X_dev, y_dev = dev_df['review'].values, dev_df['label'].values\n",
    "\n",
    "test_data_file = 'movie_reviews_test.txt'\n",
    "test_df = pd.read_csv(test_data_file, sep='\\t', header=None, names=['id', 'review', 'label'])[['review', 'label']]\n",
    "X_test, y_test = test_df['review'].values, test_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa0d2b64-25f0-4bdd-a6ff-8a8aea1c586b",
   "metadata": {
    "id": "fa0d2b64-25f0-4bdd-a6ff-8a8aea1c586b"
   },
   "outputs": [],
   "source": [
    "def preprocess_token(s): # This function is for pre-processing each token, not the entire sequence\n",
    "    # Retain only alphanumeric characters\n",
    "    s = re.sub(r'[^a-zA-Z0-9]', '', s)\n",
    "\n",
    "    # replace digits with no space\n",
    "    s = re.sub(r'\\d', '', s)\n",
    "\n",
    "    # Replace all whitespace sequences with no space\n",
    "    s = re.sub(r'\\s+', '', s)\n",
    "\n",
    "    return s\n",
    "\n",
    "def tokenize(x_train, x_dev, x_test, vocab_size): # This function is for pre-processing strings, which uses the above.\n",
    "    word_list = []\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    \n",
    "    for doc in x_train:\n",
    "        for word in doc.lower().split():\n",
    "            word = preprocess_token(word)\n",
    "            if word not in stop_words and word != '':\n",
    "                word_list.append(word)\n",
    "    corpus = Counter(word_list)\n",
    "    # Retain the 'vocab_size' most frequent words\n",
    "    corpus = sorted(corpus, key=corpus.get, reverse=True)[:vocab_size]\n",
    " \n",
    "    vocabulary = {w:i for i,w in enumerate(corpus)}\n",
    "    # tokenize\n",
    "    # Initialize empty lists to store padded sequences for training, development, and testing data\n",
    "\n",
    "    padded_seq_train, padded_seq_dev, padded_seq_test = [], [], []\n",
    "    \n",
    "    # Iterate through each document in the training data\n",
    "    for doc in x_train:\n",
    "            padded_seq_train.append([vocabulary[preprocess_token(token)] for token in doc.lower().split() \n",
    "                                     if preprocess_token(token) in vocabulary.keys()])\n",
    "            \n",
    "    # Iterate through each document in the development data\n",
    "    for doc in x_dev:\n",
    "            padded_seq_dev.append([vocabulary[preprocess_token(token)] for token in doc.lower().split() \n",
    "                                    if preprocess_token(token) in vocabulary.keys()])\n",
    " \n",
    "\n",
    "    # Iterate through each document in the testing data\n",
    "    for doc in x_test:\n",
    "            padded_seq_test.append([vocabulary[preprocess_token(token)] for token in doc.lower().split() \n",
    "                                    if preprocess_token(token) in vocabulary.keys()])\n",
    " \n",
    "    max_seq_size = max(max(max([len(s) for s in padded_seq_train]), max([len(s) for s in padded_seq_test])), max([len(s) for s in padded_seq_dev]))\n",
    "    \n",
    "    \n",
    "    for i in range(len(padded_seq_train)):\n",
    "        padded_seq_train[i] = [0]*(max_seq_size - len(padded_seq_train[i])) + padded_seq_train[i]\n",
    " \n",
    "    for i in range(len(padded_seq_test)):\n",
    "        padded_seq_test[i] = [0]*(max_seq_size - len(padded_seq_test[i])) + padded_seq_test[i]\n",
    " \n",
    "    for i in range(len(padded_seq_test)):\n",
    "        padded_seq_dev[i] = [0]*(max_seq_size - len(padded_seq_dev[i])) + padded_seq_dev[i]\n",
    "        \n",
    "        \n",
    "    # Finally, return the padded sequences (train, development and test) and vocabulary\n",
    "    return np.array(padded_seq_train), np.array(padded_seq_dev), np.array(padded_seq_test), vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "912fd19f-04b5-4549-b944-142b99be21f7",
   "metadata": {
    "id": "912fd19f-04b5-4549-b944-142b99be21f7"
   },
   "outputs": [],
   "source": [
    "# Tokenize your train, test and development data\n",
    "X_train,X_test,X_dev,vocabulary = tokenize(X_train,X_test,X_dev,vocab_size=10000)\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee92ad20-ceea-4579-ae72-03b4cb0d7b39",
   "metadata": {
    "id": "ee92ad20-ceea-4579-ae72-03b4cb0d7b39"
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train.astype(np.float32)))\n",
    "dev_data = TensorDataset(torch.from_numpy(X_dev), torch.from_numpy(y_dev.astype(np.float32)))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test.astype(np.float32)))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 25\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "dev_loader = DataLoader(dev_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dea3832-6ad4-4c9c-845b-bfbaa04dc03a",
   "metadata": {
    "id": "0dea3832-6ad4-4c9c-845b-bfbaa04dc03a"
   },
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self,num_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n",
    "        super(SentimentRNN,self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout_layer = nn.Dropout(drop_prob)\n",
    "\n",
    "        # linear and sigmoid layer\n",
    "        self.linear_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x,hidden):\n",
    "\n",
    "        # embeddings\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
    "\n",
    "        # stack up LSTM outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout_layer(lstm_out)\n",
    "        out = self.linear_layer(out)\n",
    "\n",
    "        # sigmoid function\n",
    "        sig_out = self.activation(out)\n",
    "\n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "\n",
    "        # get the last batch of labels\n",
    "        sig_out = sig_out[:, -1]\n",
    "        \n",
    "        return sig_out, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # initialize hidden state(s) and cell state(s) of LSTM to zero with appropriate dimensions\n",
    "        h0 = torch.zeros((self.num_layers, batch_size, self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.num_layers, batch_size, self.hidden_dim)).to(device)\n",
    "        hidden = (h0,c0)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76fbf614-ddc3-475a-a488-a8dddad7282b",
   "metadata": {
    "id": "76fbf614-ddc3-475a-a488-a8dddad7282b"
   },
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "vocab_size = len(vocabulary)\n",
    "embedding_dim = int(32)\n",
    "output_dim = 1\n",
    "hidden_dim = 256\n",
    "\n",
    "model = SentimentRNN(num_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8d96d5d-aa54-41e0-856f-03cd7a740029",
   "metadata": {
    "id": "b8d96d5d-aa54-41e0-856f-03cd7a740029"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suyash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "lr=0.0001\n",
    "\n",
    "# you should use binary cross-entropy as your loss function and Adam optimizer for this task\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "loss_func = nn.BCELoss()\n",
    "\n",
    "# function to predict accuracy\n",
    "def acc(preds, labels, threshold=0.5):\n",
    "    binary_predictions = (preds > threshold).float()\n",
    "    return torch.tensor(torch.sum(binary_predictions == labels).item() / len(preds))\n",
    "\n",
    "def mean(listt):\n",
    "    return sum(listt)/len(listt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b502a0ea-6570-47a6-a7b8-9d0a307d0541",
   "metadata": {
    "id": "b502a0ea-6570-47a6-a7b8-9d0a307d0541"
   },
   "outputs": [],
   "source": [
    "clip = 5\n",
    "epochs = 10\n",
    "dev_loss_min = np.Inf\n",
    "best_epoch = 0\n",
    "\n",
    "epoch_tr_loss,epoch_dv_loss = [],[]\n",
    "epoch_tr_acc,epoch_dv_acc = [],[]\n",
    "\n",
    "for epoch in range(epochs): # Train your model\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    dev_loss = []\n",
    "    dev_acc = []\n",
    "    \n",
    "    for features, target in train_loader:\n",
    "        features = features.to(device)\n",
    "        target = target.to(device)\n",
    "        hidden_state = model.init_hidden(batch_size)\n",
    "        out, _ = model(features, hidden_state)\n",
    "        loss = loss_func(out, target)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.append(loss.item())\n",
    "        train_acc.append(acc(out, target))\n",
    "    \n",
    "    for features, target in dev_loader:\n",
    "        features = features.to(device)\n",
    "        target = target.to(device)\n",
    "        hidden_state = model.init_hidden(batch_size)\n",
    "        out, _ = model(features, hidden_state)\n",
    "        loss = loss_func(out, target)\n",
    "        dev_loss.append(loss.item())\n",
    "        dev_acc.append(acc(out, target))\n",
    "    \n",
    "    mean_dev_loss = mean(dev_loss)\n",
    "    mean_train_loss = mean(train_loss)\n",
    "    mean_train_acc = mean(train_acc)\n",
    "    mean_dev_acc = mean(dev_acc)\n",
    "    \n",
    "    if (epoch+1)%2==0 or epoch==0:\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        print(f'train_loss : {mean_train_loss} dev_loss : {mean_dev_loss}')\n",
    "        print(f'train_accuracy : {mean_train_acc} dev_accuracy : {mean_dev_acc}')\n",
    "        print(25*'==')\n",
    "\n",
    "    # if dev_loss goes less than or equal to dev_loss_min then save your model and update the dev_loss_min\n",
    "\n",
    "    if mean_dev_loss<dev_loss_min:\n",
    "        dev_loss_min = mean_dev_loss\n",
    "        # save model here\n",
    "        torch.save(model.state_dict(), f'best_model.pth')\n",
    "        best_epoch = epoch+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa883eea",
   "metadata": {},
   "source": [
    "NOTE: your train loss should be smaller than 1 and your train accuracy should be over 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd036fbe-9b75-4217-add1-20dd33cc48a3",
   "metadata": {
    "id": "cd036fbe-9b75-4217-add1-20dd33cc48a3"
   },
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "test_acc = 0.0\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Iterate over test data batches\n",
    "for inputs, labels in test_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    test_h = model.init_hidden(batch_size)\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs, _ = model(inputs, test_h)\n",
    "    \n",
    "    # Compute predictions\n",
    "    predictions.extend(outputs.cpu().numpy())  # Assuming outputs are on CPU\n",
    "    true_labels.extend(labels.cpu().numpy())  # Assuming labels are on CPU\n",
    "\n",
    "# Convert predictions and true_labels to numpy arrays\n",
    "predictions = torch.tensor(predictions)\n",
    "true_labels = torch.tensor(true_labels)\n",
    "\n",
    "\n",
    "\n",
    "#########\n",
    "# Accuracy\n",
    "print(f\"F1 Score - {binary_f1_score(predictions, true_labels)}\")\n",
    "metric = BinaryAUROC()\n",
    "metric.update(predictions, true_labels)\n",
    "print(f\"Area under ROC {metric.compute()}\")\n",
    "acc_metric = BinaryAccuracy()\n",
    "acc_metric.update(predictions, true_labels)\n",
    "print(f\"Accuracy {metric.compute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f332ea53",
   "metadata": {},
   "source": [
    "NOTE: your eval accuracy should be of at least 60%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8917a512",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
