{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e28f7c45-f3f5-45b8-8b41-5012bc57d4bd",
   "metadata": {
    "id": "e28f7c45-f3f5-45b8-8b41-5012bc57d4bd"
   },
   "source": [
    "Homework 3: Sentiment Analysis\n",
    "----\n",
    "\n",
    "The following instructions apply to all notebooks and `.py` files you submit for this homework.\n",
    "\n",
    "Due date: April 15th, 2024 11:59 PM (EST)\n",
    "\n",
    "Total Points: (105)\n",
    "- Task 0: 05 points\n",
    "- Task 1: 10 points\n",
    "- Task 2: 20 points\n",
    "- Task 3: 25 points\n",
    "- Task 4: 40 points (question in LSTM_EncDec.ipynb)\n",
    "\n",
    "Goals:\n",
    "- understand the difficulties of counting and probabilities in NLP applications\n",
    "- work with real world data using different approaches to classification\n",
    "- stress test your model (to some extent)\n",
    "\n",
    "\n",
    "Allowed python modules:\n",
    "- `numpy`, `matplotlib`, `keras`, `pytorch`, `nltk`, `pandas`, `sci-kit learn` (`sklearn`), `seaborn`, and all built-in python libraries (e.g. `math` and `string`)\n",
    "- if you would like to use a library not on this list, please check with us on Campuswire first.\n",
    "- all *necessary* imports have been included for you (all imports that we used in our solution)\n",
    "\n",
    "Instructions:\n",
    "- Complete outlined problems in this notebook.\n",
    "- When you have finished, __clear the kernel__ and __run__ your notebook \"fresh\" from top to bottom. Ensure that there are __no errors__.\n",
    "    - If a problem asks for you to write code that does result in an error (as in, the answer to the problem is an error), leave the code in your notebook but commented out so that running from top to bottom does not result in any errors.\n",
    "- Double check that you have completed Task 0.\n",
    "- Submit your work on Gradescope.\n",
    "- Double check that your submission on Gradescope looks like you believe it should."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc3f1aa-6b6e-4953-a30d-a79ab0794170",
   "metadata": {
    "id": "cfc3f1aa-6b6e-4953-a30d-a79ab0794170"
   },
   "source": [
    "Names & Sections\n",
    "----\n",
    "Names: __NISHARG GOSAI__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4307e0-20b7-4ed3-9fa0-086f899b4583",
   "metadata": {
    "id": "2a4307e0-20b7-4ed3-9fa0-086f899b4583"
   },
   "source": [
    "Task 0: Name, References, Reflection (5 points)\n",
    "---\n",
    "\n",
    "References\n",
    "---\n",
    "List the resources you consulted to complete this homework here. Write one sentence per resource about what it provided to you. If you consulted no references to complete your assignment, write a brief sentence stating that this is the case and why it was the case for you.\n",
    "\n",
    "(Example)\n",
    "- https://docs.python.org/3/tutorial/datastructures.html\n",
    "    - Read about the the basics and syntax for data structures in python.\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "    - Read about the sklearn's tfidf vectorizer\n",
    "\n",
    "AI Collaboration\n",
    "---\n",
    "Following the *Policy on the use of Generative AI* in the syllabus, please cite any LLMs that you used here and briefly describe what you used them for, including to improve language clarity in the written sections.\n",
    "\n",
    "**Used chatgpt for code refactoring and beautification and as quick documentation guide for torch and other libraries**\n",
    "\n",
    "Reflection\n",
    "----\n",
    "Answer the following questions __after__ you complete this assignment (no more than 1 sentence per question required, this section is graded on completion):\n",
    "\n",
    "1. Does this work reflect your best effort?\n",
    "    **Yes**\n",
    "2. What was/were the most challenging part(s) of the assignment? **LSTM**\n",
    "3. If you want feedback, what function(s) or problem(s) would you like feedback on and why? **How to make good vectorizer**\n",
    "4. Briefly reflect on how your partnership functioned--who did which tasks, how was the workload on each of you individually as compared to the previous homeworks, etc. **individual**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c4773-456b-4d5a-bdce-df7bbcb3d16b",
   "metadata": {
    "id": "669c4773-456b-4d5a-bdce-df7bbcb3d16b"
   },
   "source": [
    "Task 1: Provided Data Write-Up (10 points)\n",
    "---\n",
    "\n",
    "Every time you use a data set in an NLP application (or in any software application), you should be able to answer a set of questions about that data. Answer these now. Default to no more than 1 sentence per question needed. If more explanation is necessary, do give it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c281567d-b99b-45df-8b1b-2b0e35cb9196",
   "metadata": {
    "id": "c281567d-b99b-45df-8b1b-2b0e35cb9196"
   },
   "source": [
    "This is about the __provided__ movie review data set.\n",
    "\n",
    "1. Where did you get the data from? The provided dataset(s) were sub-sampled from https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "2. (1 pt) How was the data collected (where did the people acquiring the data get it from and how)?\n",
    "3. (2 pts) How large is the dataset (answer for both the train and the dev set, separately)? (# reviews, # tokens in both the train and dev sets)\n",
    "4. (1 pt) What is your data? (i.e. newswire, tweets, books, blogs, etc)\n",
    "5. (1 pt) Who produced the data? (who were the authors of the text? Your answer might be a specific person or a particular group of people)\n",
    "6. (2 pts) What is the distribution of labels in the data (answer for both the train and the dev set, separately)?\n",
    "7. (2 pts) How large is the vocabulary (answer for both the train and the dev set, separately)?\n",
    "8. (1 pt) How big is the overlap between the vocabulary for the train and dev set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d64fd8f-f130-4f1b-8624-c151c502344b",
   "metadata": {
    "id": "2d64fd8f-f130-4f1b-8624-c151c502344b"
   },
   "source": [
    "Task 2: Train a Logistic Regression Model (20 points)\n",
    "----\n",
    "1. Implement a custom function to read in a dataset, and return a list of tuples, using the Tf-Idf feature extraction technique.\n",
    "2. Compare your implementation to `sklearn`'s TfidfVectorizer (imported below) by timing both on the provided datasets using the time module.\n",
    "3. Using each set of features, and `sklearn`'s implementation of `LogisticRegression`, train a machine learning model to predict sentiment on the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e9bbb93-9a5d-4326-8dd0-d2ffcf242123",
   "metadata": {
    "id": "0e9bbb93-9a5d-4326-8dd0-d2ffcf242123"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62c515ae-3637-42b6-9412-e0710e7821bb",
   "metadata": {
    "id": "62c515ae-3637-42b6-9412-e0710e7821bb"
   },
   "outputs": [],
   "source": [
    "# The following function reads a data-file and splits the contents by tabs.\n",
    "# The first column is an ID, and thus is discarded. The second column consists of the actual reviews data.\n",
    "# The third column is the true label for each data point.\n",
    "\n",
    "# The function returns two objects - a list of all reviews, and a numpy array of labels.\n",
    "# You will need to use this function later.\n",
    "\n",
    "def get_lists(input_file):\n",
    "    f=open(input_file, 'r')\n",
    "    lines = [line.split('\\t')[1:] for line in f.readlines()]\n",
    "    X = [row[0] for row in lines]\n",
    "    y=np.array([int(row[1]) for row in lines])\n",
    "    return X, y\n",
    "\n",
    "# Fill in the following function to take a corpus (list of reviews) as input,\n",
    "# extract TfIdf values and return an array of features and the vocabulary.\n",
    "\n",
    "# If the vocabulary argument is supplied, then the function should only convert the input corpus\n",
    "# to feature vectors using the provided vocabulary and the max_features argument (if not None).\n",
    "# In this case, the function should return feature vectors and the supplied vocabulary.\n",
    "\n",
    "# If the max_features parameter is set to None, then all words in the corpus should be used.\n",
    "# If the max_features parameter is specified (say, k),\n",
    "# then only use the k most frequent words in the corpus to build your vocabulary.\n",
    "\n",
    "# The function should return two things.\n",
    "\n",
    "# The first object should be a numpy array of shape (n_documents, vocab_size),\n",
    "# which contains the TF-IDF feature vectors for each document.\n",
    "\n",
    "# The second object should be a dictionary of the words in the vocabulary,\n",
    "# mapped to their corresponding index in alphabetical sorted order.\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def get_tfidf_vectors(token_lists, max_features=None, vocabulary=None):\n",
    "    if vocabulary is None:\n",
    "        vocabulary = set(word for tokens in token_lists for word in tokens)\n",
    "\n",
    "    if max_features is not None and len(vocabulary) > max_features:\n",
    "        term_freq = defaultdict(int)\n",
    "        for tokens in token_lists:\n",
    "            for token in set(tokens):\n",
    "                term_freq[token] += 1\n",
    "        vocabulary = sorted(vocabulary, key=lambda x: term_freq[x], reverse=True)[:max_features]\n",
    "\n",
    "    vocab_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "    tfidf_matrix = csr_matrix((len(token_lists), len(vocabulary)), dtype=np.float32)\n",
    "\n",
    "    idf = {word: np.log(len(token_lists) / (1 + sum(1 for tokens in token_lists if word in tokens))) for word in vocabulary}\n",
    "\n",
    "    for idx, tokens in enumerate(token_lists):\n",
    "        total_tokens = len(tokens)\n",
    "        for token in set(tokens):\n",
    "            if token in vocab_index:\n",
    "                tfidf_matrix[idx, vocab_index[token]] = (tokens.count(token) / total_tokens) * idf[token]\n",
    "\n",
    "    return tfidf_matrix, vocab_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62226f12-d0e5-4e74-887e-5cd852b96707",
   "metadata": {
    "id": "62226f12-d0e5-4e74-887e-5cd852b96707"
   },
   "source": [
    "We will now compare the runtime of our Tf-Idf implementation to the `sklearn` implementation. Call the respective functions with appropriate arguments in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e6c726-0e12-4e1f-8c8b-146a9a506a3f",
   "metadata": {
    "id": "e2e6c726-0e12-4e1f-8c8b-146a9a506a3f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom TF-IDF - Time taken: 19.674981594085693 seconds\n",
      "scikit-learn TF-IDF - Time taken: 0.17102265357971191 seconds\n"
     ]
    }
   ],
   "source": [
    "# define constants for the files we are using\n",
    "TRAIN_FILE = \"movie_reviews_train.txt\"\n",
    "TEST_FILE = \"movie_reviews_test.txt\"\n",
    "\n",
    "train_corpus, y_train = get_lists(TRAIN_FILE)\n",
    "\n",
    "# First we will use our custom vectorizer to convert words to features, and time it.\n",
    "\n",
    "start = time.time()\n",
    "tfidf_matrix_custom, vocab_custom = get_tfidf_vectors(train_corpus)\n",
    "end = time.time()\n",
    "print(\"Custom TF-IDF - Time taken:\", end - start, \"seconds\")\n",
    "\n",
    "# print(\"Time taken: \", end-start, \" seconds\")\n",
    "\n",
    "# Next we will use sklearn's TfidfVectorizer to load in the data, and time it.\n",
    "\n",
    "start = time.time()\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix_sklearn = vectorizer.fit_transform(train_corpus)\n",
    "end = time.time()\n",
    "print(\"scikit-learn TF-IDF - Time taken:\", end - start, \"seconds\")\n",
    "\n",
    "# print(\"Time taken: \", end-start, \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35afb9c6",
   "metadata": {},
   "source": [
    "NOTE: Ideally, your vectorizer should be within one order of magnitude of the sklearn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bdbf31c-bf6b-4ab4-b87b-47dd7c5468a8",
   "metadata": {
    "id": "8bdbf31c-bf6b-4ab4-b87b-47dd7c5468a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Vocabulary Size: 125\n",
      "scikit-learn Vocabulary Size: 22684\n",
      "Custom Features Sparsity: 64.113\n",
      "scikit-learn Features Sparsity: 99.39492318374185\n"
     ]
    }
   ],
   "source": [
    "# Any additional code needed to answer questions below.\n",
    "vocab_size_custom = len(vocab_custom)\n",
    "print(\"Custom Vocabulary Size:\", vocab_size_custom)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix_sklearn = vectorizer.fit_transform(train_corpus)\n",
    "vocab_size_sklearn = len(vectorizer.vocabulary_)\n",
    "print(\"scikit-learn Vocabulary Size:\", vocab_size_sklearn)\n",
    "\n",
    "#sparsity calculation\n",
    "def calculate_sparsity(matrix):\n",
    "    total_elements = matrix.shape[0] * matrix.shape[1]\n",
    "    zero_elements = total_elements - np.sum(matrix != 0)  \n",
    "    sparsity = (zero_elements / total_elements) * 100\n",
    "    return sparsity\n",
    "\n",
    "sparsity_custom = calculate_sparsity(tfidf_matrix_custom)\n",
    "print(\"Custom Features Sparsity:\", sparsity_custom)\n",
    "\n",
    "sparsity_sklearn = calculate_sparsity(tfidf_matrix_sklearn.toarray())\n",
    "print(\"scikit-learn Features Sparsity:\", sparsity_sklearn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6463f022-7433-4397-88ed-eed8b01b1a45",
   "metadata": {
    "id": "6463f022-7433-4397-88ed-eed8b01b1a45"
   },
   "source": [
    "1. How large is the vocabulary generated by your vectorizer?<br> **125**\n",
    "2. How large is the vocabulary generated by the `sklearn` TfidfVectorizer?<br>**26373**\n",
    "3. Where might these differences be coming from?<br> **Scikit-learn's more sophisticated tokenization, preprocessing, and feature extraction techniques, resulting in a larger vocabulary that captures more unique terms and variations in the text data.**\n",
    "4. What steps did you take to ensure your vectorizer is optimized for best possible runtime?<br> **sparse matrix and precomputed idf values**\n",
    "5. How sparse are your custom features (average percentage of features per review that are zero)?<br> **64.11%**\n",
    "6. How sparse are the TfidfVectorizer's features?<br> **99.46%**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f496b",
   "metadata": {},
   "source": [
    "NOTE: if you set the lowercase option to False, the sklearn vectorizer should have a vocabulary of around 50k words/tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c76612-4dd4-489c-90e4-0c38029c32d1",
   "metadata": {
    "id": "09c76612-4dd4-489c-90e4-0c38029c32d1"
   },
   "source": [
    "**Logistic Regression**\n",
    "\n",
    "Now, we will compare how our custom features stack up against sklearn's TfidfVectorizer, by training two separate Logistic Regression classifiers - one on each set of feature vectors. Then load the test set, and convert it to two sets of feature vectors, one using our custom vectorizer (to do this, provide the vocabulary as a function argument), and one using sklearn's Tfidf (use the same object as before to transform the test inputs). For both classifiers, print the average accuracy on the test set and the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68eb6d36-1e71-4665-aab4-1217a42e6dcc",
   "metadata": {
    "id": "68eb6d36-1e71-4665-aab4-1217a42e6dcc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Features - Accuracy: 0.545\n",
      "Custom Features - F1 Score: 0.7055016181229774\n",
      "scikit-learn Features - Accuracy: 0.775\n",
      "scikit-learn Features - F1 Score: 0.7906976744186047\n"
     ]
    }
   ],
   "source": [
    "# First use sklearn's LogisticRegression classifier to do sentiment analysis using your custom feature vectors:\n",
    "\n",
    "lr_custom = LogisticRegression()\n",
    "lr_custom.fit(tfidf_matrix_custom, y_train)\n",
    "\n",
    "# Load the test data, extract features using your custom vectorizer, and test the performance of the LR classifier\n",
    "\n",
    "test_corpus, y_test = get_lists(TEST_FILE)\n",
    "tfidf_matrix_test_custom, _ = get_tfidf_vectors(test_corpus, vocabulary=vocab_custom)\n",
    "y_pred_custom = lr_custom.predict(tfidf_matrix_test_custom)\n",
    "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
    "f1_custom = f1_score(y_test, y_pred_custom)\n",
    "\n",
    "# Print the accuracy of your model on the test data\n",
    "\n",
    "print(\"Custom Features - Accuracy:\", accuracy_custom)\n",
    "print(\"Custom Features - F1 Score:\", f1_custom)\n",
    "\n",
    "\n",
    "# Now repeat the above steps, but this time using features extracted by sklearn's Tfidfvectorizer\n",
    "\n",
    "\n",
    "lr_sklearn = LogisticRegression()\n",
    "lr_sklearn.fit(tfidf_matrix_sklearn, y_train)\n",
    "tfidf_matrix_test_sklearn = vectorizer.transform(test_corpus)\n",
    "y_pred_sklearn = lr_sklearn.predict(tfidf_matrix_test_sklearn)\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "f1_sklearn = f1_score(y_test, y_pred_sklearn)\n",
    "print(\"scikit-learn Features - Accuracy:\", accuracy_sklearn)\n",
    "print(\"scikit-learn Features - F1 Score:\", f1_sklearn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5679888",
   "metadata": {},
   "source": [
    "NOTE: we're expecting to see a F1 score of around 80% using both your custom features and the sklearn features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0220a93-ba22-411a-962f-983ffe3a34f2",
   "metadata": {
    "id": "f0220a93-ba22-411a-962f-983ffe3a34f2"
   },
   "source": [
    "Finally, repeat the process (training and testing), but this time, set the max_features argument to 1000 for both our custom vectorizer and sklearn's Tfidfvectorizer. Report average accuracy and F1 scores for both classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f995a09c-0447-422d-8b37-e9120cfadfab",
   "metadata": {
    "id": "f995a09c-0447-422d-8b37-e9120cfadfab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Features (max_features=1000) - Accuracy: 0.545\n",
      "Custom Features (max_features=1000) - F1 Score: 0.7055016181229774\n",
      "scikit-learn Features (max_features=1000) - Accuracy: 0.785\n",
      "scikit-learn Features (max_features=1000) - F1 Score: 0.8036529680365296\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# First use sklearn's LogisticRegression classifier to do sentiment analysis using your custom feature vectors:\n",
    "\n",
    "tfidf_matrix_custom_1000, vocab_custom_1000 = get_tfidf_vectors(train_corpus, max_features=1000)\n",
    "lr_custom_1000 = LogisticRegression()\n",
    "lr_custom_1000.fit(tfidf_matrix_custom_1000, y_train)\n",
    "\n",
    "\n",
    "# Load the test data, extract features using your custom vectorizer, and test the performance of the LR classifier\n",
    "\n",
    "# Extract features using your custom vectorizer with max_features=1000\n",
    "tfidf_matrix_test_custom_1000, _ = get_tfidf_vectors(test_corpus, vocabulary=vocab_custom_1000, max_features=1000)\n",
    "\n",
    "# Test the performance of the LR classifier on custom features with max_features=1000\n",
    "y_pred_custom_1000 = lr_custom_1000.predict(tfidf_matrix_test_custom_1000)\n",
    "accuracy_custom_1000 = accuracy_score(y_test, y_pred_custom_1000)\n",
    "f1_custom_1000 = f1_score(y_test, y_pred_custom_1000)\n",
    "\n",
    "\n",
    "# Print the accuracy of your model on the test data\n",
    "print(\"Custom Features (max_features=1000) - Accuracy:\", accuracy_custom_1000)\n",
    "print(\"Custom Features (max_features=1000) - F1 Score:\", f1_custom_1000)\n",
    "\n",
    "# Now repeat the above steps, but this time using features extracted by sklearn's Tfidfvectorizer\n",
    "\n",
    "###### YOUR CODE HERE #######\n",
    "vectorizer_1000 = TfidfVectorizer(max_features=1000)\n",
    "tfidf_matrix_sklearn_1000 = vectorizer_1000.fit_transform(train_corpus)\n",
    "lr_sklearn_1000 = LogisticRegression()\n",
    "lr_sklearn_1000.fit(tfidf_matrix_sklearn_1000, y_train)\n",
    "tfidf_matrix_test_sklearn_1000 = vectorizer_1000.transform(test_corpus)\n",
    "y_pred_sklearn_1000 = lr_sklearn_1000.predict(tfidf_matrix_test_sklearn_1000)\n",
    "accuracy_sklearn_1000 = accuracy_score(y_test, y_pred_sklearn_1000)\n",
    "f1_sklearn_1000 = f1_score(y_test, y_pred_sklearn_1000)\n",
    "print(\"scikit-learn Features (max_features=1000) - Accuracy:\", accuracy_sklearn_1000)\n",
    "print(\"scikit-learn Features (max_features=1000) - F1 Score:\", f1_sklearn_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2858e409-8a87-43aa-8646-1e5419cce6db",
   "metadata": {
    "id": "2858e409-8a87-43aa-8646-1e5419cce6db"
   },
   "source": [
    "1. Is there a stark difference between the two vectorizers with 1000 features?<br>**The scikit-learn TfidfVectorizer seems to consistently outperform the custom TF-IDF vectorizer in terms of accuracy and F1 score, even when both are limited to 1000 features.**\n",
    "2. Use sklearn's documentation for the Tfidfvectorizer to figure out what may be causing the performance difference (or lack thereof).<br>**Performance variations may arise from differences in tokenization and parameter tuning. Tweaking these aspects could reduce performance gaps.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68df8d77",
   "metadata": {},
   "source": [
    "NOTE: Irrespective of your conclusions, both implementations should be above 60% F1 Score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c652f8ab-3893-4021-b33f-c466b2e1d98a",
   "metadata": {
    "id": "c652f8ab-3893-4021-b33f-c466b2e1d98a"
   },
   "source": [
    "Task 3: Train a Feedforward Neural Network Model (25 points)\n",
    "----\n",
    "1. Using PyTorch, implement a feedforward neural network to do sentiment analysis. This model should take sparse vectors of length 10000 as input (note this is 10000, not 1000), and have a single output with the sigmoid activation function. The number of hidden layers, and intermediate activation choices are up to you, but please make sure your model does not take more than ~1 minute to train.\n",
    "2. Evaluate the model using PyTorch functions for average accuracy, area under the ROC curve and F1 scores (see [torcheval](https://pytorch.org/torcheval/stable/)) using both vectorizers, with max_features set to 10000 in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f563ec2-d02a-4176-908c-011acd8851de",
   "metadata": {
    "id": "3f563ec2-d02a-4176-908c-011acd8851de"
   },
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from torcheval.metrics.functional import binary_f1_score\n",
    "from torcheval.metrics import BinaryAUROC, BinaryAccuracy\n",
    "\n",
    "\n",
    "# if torch.backends.mps.is_available():\n",
    "# \tdevice = torch.device(\"mps\")\n",
    "if torch.cuda.is_available():\n",
    "\tdevice = torch.device(\"cuda\")\n",
    "else:\n",
    "\tdevice = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcb6aa45-e33a-4d08-bff8-996e1f80dad2",
   "metadata": {
    "id": "fcb6aa45-e33a-4d08-bff8-996e1f80dad2"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.forward(x)\n",
    "            predicted = torch.round(outputs)\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba08e134-2a81-48b9-89c3-43046be4cc1b",
   "metadata": {
    "id": "ba08e134-2a81-48b9-89c3-43046be4cc1b"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Load the data\n",
    "X, y = get_lists(\"movie_reviews_train.txt\")\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "\n",
    "# Fit and transform the data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to create a PyTorch DataLoader\n",
    "def create_data_loader(X, y, batch_size=64):\n",
    "    tensor_x = torch.Tensor(X.toarray())  \n",
    "    tensor_y = torch.Tensor(y).unsqueeze(1)\n",
    "    dataset = TensorDataset(tensor_x, tensor_y) \n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Prepare DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = create_data_loader(X_train, y_train, batch_size)\n",
    "test_loader = create_data_loader(X_test, y_test, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "055e1e59-4d96-4a6b-8e16-cd14fcc4b760",
   "metadata": {
    "id": "055e1e59-4d96-4a6b-8e16-cd14fcc4b760"
   },
   "outputs": [],
   "source": [
    "# Create a feedforward neural network model\n",
    "# you may use any activation function on the hidden layers\n",
    "# you should use binary cross-entropy as your loss function\n",
    "# Adam is an appropriate optimizer for this task\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.forward(x)\n",
    "            predicted = torch.round(outputs)\n",
    "        return predicted\n",
    "\n",
    "# Define the model with the correct input size\n",
    "input_size = 10000  \n",
    "model = FeedForward(input_size)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "237cbc15-3473-427d-8d3f-af1059f736b4",
   "metadata": {
    "id": "237cbc15-3473-427d-8d3f-af1059f736b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Custom Loss: 0.6735047101974487\n",
      "Epoch [2/50], Custom Loss: 0.5949637293815613\n",
      "Epoch [3/50], Custom Loss: 0.48349279165267944\n",
      "Epoch [4/50], Custom Loss: 0.3643815517425537\n",
      "Epoch [5/50], Custom Loss: 0.25304919481277466\n",
      "Epoch [6/50], Custom Loss: 0.16466936469078064\n",
      "Epoch [7/50], Custom Loss: 0.10459190607070923\n",
      "Epoch [8/50], Custom Loss: 0.08420170843601227\n",
      "Epoch [9/50], Custom Loss: 0.06560181826353073\n",
      "Epoch [10/50], Custom Loss: 0.04548829048871994\n",
      "Epoch [11/50], Custom Loss: 0.038836829364299774\n",
      "Epoch [12/50], Custom Loss: 0.02514059841632843\n",
      "Epoch [13/50], Custom Loss: 0.01968236081302166\n",
      "Epoch [14/50], Custom Loss: 0.016673708334565163\n",
      "Epoch [15/50], Custom Loss: 0.017924942076206207\n",
      "Epoch [16/50], Custom Loss: 0.01413821056485176\n",
      "Epoch [17/50], Custom Loss: 0.012358732521533966\n",
      "Epoch [18/50], Custom Loss: 0.009591622278094292\n",
      "Epoch [19/50], Custom Loss: 0.008305499330163002\n",
      "Epoch [20/50], Custom Loss: 0.0065590087324380875\n",
      "Epoch [21/50], Custom Loss: 0.007293409202247858\n",
      "Epoch [22/50], Custom Loss: 0.0062445830553770065\n",
      "Epoch [23/50], Custom Loss: 0.006763174198567867\n",
      "Epoch [24/50], Custom Loss: 0.005701232701539993\n",
      "Epoch [25/50], Custom Loss: 0.005461477674543858\n",
      "Epoch [26/50], Custom Loss: 0.003510041395202279\n",
      "Epoch [27/50], Custom Loss: 0.0046006436459720135\n",
      "Epoch [28/50], Custom Loss: 0.0037166145630180836\n",
      "Epoch [29/50], Custom Loss: 0.0038346266373991966\n",
      "Epoch [30/50], Custom Loss: 0.0027983991894870996\n",
      "Epoch [31/50], Custom Loss: 0.003303611185401678\n",
      "Epoch [32/50], Custom Loss: 0.002740314928814769\n",
      "Epoch [33/50], Custom Loss: 0.002485903212800622\n",
      "Epoch [34/50], Custom Loss: 0.0024308511056005955\n",
      "Epoch [35/50], Custom Loss: 0.0025523845106363297\n",
      "Epoch [36/50], Custom Loss: 0.002253189217299223\n",
      "Epoch [37/50], Custom Loss: 0.002271754201501608\n",
      "Epoch [38/50], Custom Loss: 0.00200076075270772\n",
      "Epoch [39/50], Custom Loss: 0.0018085222691297531\n",
      "Epoch [40/50], Custom Loss: 0.00185445137321949\n",
      "Epoch [41/50], Custom Loss: 0.0017199884168803692\n",
      "Epoch [42/50], Custom Loss: 0.0017646627966314554\n",
      "Epoch [43/50], Custom Loss: 0.0014613353414461017\n",
      "Epoch [44/50], Custom Loss: 0.0015057814307510853\n",
      "Epoch [45/50], Custom Loss: 0.001369953854009509\n",
      "Epoch [46/50], Custom Loss: 0.0010859596077352762\n",
      "Epoch [47/50], Custom Loss: 0.0014539252733811736\n",
      "Epoch [48/50], Custom Loss: 0.0013231346383690834\n",
      "Epoch [49/50], Custom Loss: 0.0011298401514068246\n",
      "Epoch [50/50], Custom Loss: 0.00116157962474972\n",
      "Epoch [1/50], Sklearn Loss: 0.0008886184077709913\n",
      "Epoch [2/50], Sklearn Loss: 0.0008548918995074928\n",
      "Epoch [3/50], Sklearn Loss: 0.001051114872097969\n",
      "Epoch [4/50], Sklearn Loss: 0.0009052449022419751\n",
      "Epoch [5/50], Sklearn Loss: 0.0009681893861852586\n",
      "Epoch [6/50], Sklearn Loss: 0.0007893441361375153\n",
      "Epoch [7/50], Sklearn Loss: 0.0008435589261353016\n",
      "Epoch [8/50], Sklearn Loss: 0.0008969220798462629\n",
      "Epoch [9/50], Sklearn Loss: 0.0008410686859861016\n",
      "Epoch [10/50], Sklearn Loss: 0.0007489431300200522\n",
      "Epoch [11/50], Sklearn Loss: 0.0007674712687730789\n",
      "Epoch [12/50], Sklearn Loss: 0.0009239144856110215\n",
      "Epoch [13/50], Sklearn Loss: 0.0006336976657621562\n",
      "Epoch [14/50], Sklearn Loss: 0.0006992952548898757\n",
      "Epoch [15/50], Sklearn Loss: 0.0007427973905578256\n",
      "Epoch [16/50], Sklearn Loss: 0.0006616186583414674\n",
      "Epoch [17/50], Sklearn Loss: 0.0006152206333354115\n",
      "Epoch [18/50], Sklearn Loss: 0.0005514936055988073\n",
      "Epoch [19/50], Sklearn Loss: 0.0005367325502447784\n",
      "Epoch [20/50], Sklearn Loss: 0.0005631205858662724\n",
      "Epoch [21/50], Sklearn Loss: 0.0005302416393533349\n",
      "Epoch [22/50], Sklearn Loss: 0.000573656870983541\n",
      "Epoch [23/50], Sklearn Loss: 0.000491524173412472\n",
      "Epoch [24/50], Sklearn Loss: 0.00043625669786706567\n",
      "Epoch [25/50], Sklearn Loss: 0.0003789636248257011\n",
      "Epoch [26/50], Sklearn Loss: 0.0004527332785073668\n",
      "Epoch [27/50], Sklearn Loss: 0.0004185998986940831\n",
      "Epoch [28/50], Sklearn Loss: 0.0005058005917817354\n",
      "Epoch [29/50], Sklearn Loss: 0.0005043668788857758\n",
      "Epoch [30/50], Sklearn Loss: 0.00038255006074905396\n",
      "Epoch [31/50], Sklearn Loss: 0.0003846537147182971\n",
      "Epoch [32/50], Sklearn Loss: 0.0004496319452300668\n",
      "Epoch [33/50], Sklearn Loss: 0.0003786708402913064\n",
      "Epoch [34/50], Sklearn Loss: 0.00036951882066205144\n",
      "Epoch [35/50], Sklearn Loss: 0.00036433522473089397\n",
      "Epoch [36/50], Sklearn Loss: 0.0003120795590803027\n",
      "Epoch [37/50], Sklearn Loss: 0.00033274840097874403\n",
      "Epoch [38/50], Sklearn Loss: 0.0002814526087604463\n",
      "Epoch [39/50], Sklearn Loss: 0.00032949078013189137\n",
      "Epoch [40/50], Sklearn Loss: 0.0003232181188650429\n",
      "Epoch [41/50], Sklearn Loss: 0.0003300970420241356\n",
      "Epoch [42/50], Sklearn Loss: 0.00030186475487425923\n",
      "Epoch [43/50], Sklearn Loss: 0.00029600015841424465\n",
      "Epoch [44/50], Sklearn Loss: 0.0003438632993493229\n",
      "Epoch [45/50], Sklearn Loss: 0.0002620044688228518\n",
      "Epoch [46/50], Sklearn Loss: 0.000255238643148914\n",
      "Epoch [47/50], Sklearn Loss: 0.0002933857904281467\n",
      "Epoch [48/50], Sklearn Loss: 0.0002629855298437178\n",
      "Epoch [49/50], Sklearn Loss: 0.0002809079014696181\n",
      "Epoch [50/50], Sklearn Loss: 0.0002376101620029658\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 50 epochs on both custom and sklearn vectors\n",
    "num_epochs = 50\n",
    "\n",
    "# Train the model on custom vectors\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Custom Loss: {loss.item()}\")\n",
    "\n",
    "# Train the model on sklearn vectors\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Sklearn Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdf1319f-41c0-4237-982f-b60d492fbbd7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdf1319f-41c0-4237-982f-b60d492fbbd7",
    "outputId": "187d0ec2-1f99-417e-b23d-de234adb2848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Custom Model:\n",
      "Custom Model - F1 Score: 0.8085\n",
      "Custom Model - AUROC: 0.8851\n",
      "Custom Model - Accuracy: 0.8031\n",
      "Evaluating Sklearn Model:\n",
      "Sklearn Model - F1 Score: 0.8085\n",
      "Sklearn Model - AUROC: 0.8851\n",
      "Sklearn Model - Accuracy: 0.8031\n"
     ]
    }
   ],
   "source": [
    "#!pip install torcheval\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torcheval.metrics.functional import binary_f1_score\n",
    "from torcheval.metrics import BinaryAUROC, BinaryAccuracy\n",
    "\n",
    "# Helper function to evaluate the model\n",
    "def evaluate_model_metrics(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            predicted = outputs.round().squeeze()  \n",
    "            y_true.extend(labels.squeeze().cpu().numpy())  \n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_scores.extend(outputs.squeeze().cpu().numpy())  \n",
    "\n",
    "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
    "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
    "    y_scores = torch.tensor(y_scores, dtype=torch.float32)\n",
    "\n",
    "    # Calculate F1 Score\n",
    "    f1 = binary_f1_score(y_pred, y_true)  \n",
    "\n",
    "    # Calculate AUROC\n",
    "    auroc = BinaryAUROC()\n",
    "    auroc.update(y_scores, y_true.int())\n",
    "    auroc_score = auroc.compute()\n",
    "\n",
    "    # Calculate Binary Accuracy\n",
    "    accuracy = BinaryAccuracy()\n",
    "    accuracy.update(y_pred, y_true)\n",
    "    accuracy_score = accuracy.compute()\n",
    "\n",
    "    return f1.item(), auroc_score.item(), accuracy_score.item()\n",
    "\n",
    "# Evaluate Custom Model\n",
    "print(\"Evaluating Custom Model:\")\n",
    "f1_custom, auroc_custom, accuracy_custom = evaluate_model_metrics(model, test_loader)\n",
    "print(f\"Custom Model - F1 Score: {f1_custom:.4f}\")\n",
    "print(f\"Custom Model - AUROC: {auroc_custom:.4f}\")\n",
    "print(f\"Custom Model - Accuracy: {accuracy_custom:.4f}\")\n",
    "\n",
    "# Evaluate Sklearn Model\n",
    "print(\"Evaluating Sklearn Model:\")\n",
    "f1_sklearn, auroc_sklearn, accuracy_sklearn = evaluate_model_metrics(model, test_loader) \n",
    "print(f\"Sklearn Model - F1 Score: {f1_sklearn:.4f}\")\n",
    "print(f\"Sklearn Model - AUROC: {auroc_sklearn:.4f}\")\n",
    "print(f\"Sklearn Model - Accuracy: {accuracy_sklearn:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a332b6",
   "metadata": {},
   "source": [
    "NOTE: As in the last task, we're expecting to see a F1 score of over 60% using both your custom features and the sklearn features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b269ee9",
   "metadata": {
    "id": "8b269ee9"
   },
   "source": [
    "5 points in this assignment are reserved for overall style (both for writing and for code submitted). All work submitted should be clear, easily interpretable, and checked for spelling, etc. (Re-read what you write and make sure it makes sense). Course staff are always happy to give grammatical help (but we won't pre-grade the content of your answers)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
