{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Transfer Learning and BERT!\n",
    "\n",
    "Created by: Ankit Ramakrishnan, Harshitha Somala, Nidhi Bodar, Felix Muzny\n",
    "\n",
    "No Submission Required\n",
    "\n",
    "Agenda\n",
    "------\n",
    "+ Get an overview of the BERT architecture\n",
    "+ Download and play with a pre-trained BERT model\n",
    "+ Train your own network that has a pre-trained BERT component\n",
    "+ Play with some parameters!\n",
    "\n",
    "\n",
    "Summary\n",
    "----\n",
    "This lab will guide you through the setup and uses of pretrained models in `Tensorflow` and `transformers`. In this lab we focus on BERT (https://github.com/google-research/bert/) an __encoder only transformer model__. We will use the pretrained model to extract features from text and use these features to train a classifier to classify different types of data. We will setup a binary classifier on the IMDB dataset. You will be working on training new models on the other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 1: BERT\n",
    "---\n",
    "\n",
    "Before we dive into the code, we'd like you to take a moment to familiarize yourself with the architecture of BERT.\n",
    "\n",
    "Take a look at the following resources as a group:\n",
    "- [Illustrated BERT](http://jalammar.github.io/illustrated-bert/)\n",
    "- SLP Chapter 11.1\n",
    "- [BERT paper](https://aclanthology.org/N19-1423/)\n",
    "\n",
    "Answer the questions in the next cell about the BERT architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What does BERT stand for? __YOUR ANSWER HERE__\n",
    "2. What task is the BERT pre-trained model trained on? __YOUR ANSWER HERE__\n",
    "3. How many parameters does BERT_{BASE} have? BERT_{LARGE}?   __YOUR ANSWER HERE__\n",
    "4. What is the architecture of BERT_{BASE}? (what kind of neural network is it, what are the relevant dimensionality numbers?)  __YOUR ANSWER HERE__\n",
    "5. What can we feed as input to BERT?  __YOUR ANSWER HERE__\n",
    "6. What is the output of BERT?  __YOUR ANSWER HERE__\n",
    "7. If we want to use BERT to help with a classification task, what is a high-level description of what we'll do? (2 - 3 sentences.)  __YOUR ANSWER HERE__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L1C-vVvT3cEj",
    "outputId": "d5f4c5b8-af8b-49aa-8bc0-8d5ff57c1f81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.35.0\n",
      "  Obtaining dependency information for transformers==4.35.0 from https://files.pythonhosted.org/packages/9a/06/e4ec2a321e57c03b7e9345d709d554a52c33760e5015fdff0919d9459af0/transformers-4.35.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.1/123.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow==2.14.0\n",
      "  Obtaining dependency information for tensorflow==2.14.0 from https://files.pythonhosted.org/packages/de/ea/90267db2c02fb61f4d03b9645c7446d3cbca6d5c08522e889535c88edfcd/tensorflow-2.14.0-cp311-cp311-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading tensorflow-2.14.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: filelock in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from transformers==4.35.0) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.35.0)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/89/66/1e0799583d8c844b59aa1a1d06ba26d50e8748d8b61b3ba8cbe4a0b26bc0/huggingface_hub-0.22.0-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.22.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from transformers==4.35.0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from transformers==4.35.0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from transformers==4.35.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from transformers==4.35.0) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from transformers==4.35.0) (2.31.0)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers==4.35.0)\n",
      "  Obtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/7e/8e/9c0f7799da9a690ec29a7a7b6c0744d3f735e40951d2f62c8202faf3df6a/tokenizers-0.14.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading tokenizers-0.14.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from transformers==4.35.0) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from transformers==4.35.0) (4.65.0)\n",
      "Collecting tensorflow-macos==2.14.0 (from tensorflow==2.14.0)\n",
      "  Obtaining dependency information for tensorflow-macos==2.14.0 from https://files.pythonhosted.org/packages/d3/4b/ae9037ea22ba94eb2cf267e991384c3444f3e6142fa49923352b4ab73e14/tensorflow_macos-2.14.0-cp311-cp311-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading tensorflow_macos-2.14.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.9 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-macos==2.14.0->tensorflow==2.14.0)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-macos==2.14.0->tensorflow==2.14.0)\n",
      "  Obtaining dependency information for astunparse>=1.6.0 from https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow-macos==2.14.0->tensorflow==2.14.0)\n",
      "  Obtaining dependency information for flatbuffers>=23.5.26 from https://files.pythonhosted.org/packages/bf/45/c961e3cb6ddad76b325c163d730562bb6deb1ace5acbed0306f5fbefb90e/flatbuffers-24.3.7-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-24.3.7-py2.py3-none-any.whl.metadata (849 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-macos==2.14.0->tensorflow==2.14.0)\n",
      "  Obtaining dependency information for gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 from https://files.pythonhosted.org/packages/fa/39/5aae571e5a5f4de9c3445dae08a530498e5c53b0e74410eeeb0991c79047/gast-0.5.4-py3-none-any.whl.metadata\n",
      "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-macos==2.14.0->tensorflow==2.14.0)\n",
      "  Obtaining dependency information for google-pasta>=0.1.1 from https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl.metadata\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14.0) (3.9.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-macos==2.14.0->tensorflow==2.14.0)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/db/ed/1df62b44db2583375f6a8a5e2ca5432bbdc3edb477942b9b7c848c720055/libclang-18.1.1-py2.py3-none-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading libclang-18.1.1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes==0.2.0 (from tensorflow-macos==2.14.0->tensorflow==2.14.0)\n",
      "  Obtaining dependency information for ml-dtypes==0.2.0 from https://files.pythonhosted.org/packages/15/da/43bee505963da0c730ee50e951c604bfdb90d4cccc9c0044c946b10e68a7/ml_dtypes-0.2.0-cp311-cp311-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading ml_dtypes-0.2.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-macos==2.14.0->tensorflow==2.14.0)\n",
      "  Obtaining dependency information for opt-einsum>=2.3.2 from https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl.metadata\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-macos==2.14.0->tensorflow==2.14.0)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/f3/bf/26deba06a4c910a85f78245cac7698f67cedd7efe00d04f6b3e1b3506a59/protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14.0) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14.0) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-macos==2.14.0->tensorflow==2.14.0)\n",
      "  Obtaining dependency information for termcolor>=1.1.0 from https://files.pythonhosted.org/packages/d9/5f/8c716e47b3a50cbd7c146f45881e11d9414def768b7cd9c5e6650ec2a80a/termcolor-2.4.0-py3-none-any.whl.metadata\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14.0) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow==2.14.0) (1.14.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-macos==2.14.0->tensorflow==2.14.0)\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/3e/56/1b7ef816e448464a93da70296db237129910b4452d6b4582d5e23fb07880/tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (14 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-macos==2.14.0->tensorflow==2.14.0)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/c1/0a/a8c0f403b2189f5d3e490778ead51924b56fa30a35f6e444b3702e28c8c8/grpcio-1.62.1-cp311-cp311-macosx_10_10_universal2.whl.metadata\n",
      "  Downloading grpcio-1.62.1-cp311-cp311-macosx_10_10_universal2.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.15,>=2.14 (from tensorflow-macos==2.14.0->tensorflow==2.14.0)\n",
      "  Obtaining dependency information for tensorboard<2.15,>=2.14 from https://files.pythonhosted.org/packages/73/a2/66ed644f6ed1562e0285fcd959af17670ea313c8f331c46f79ee77187eb9/tensorboard-2.14.1-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow-macos==2.14.0->tensorflow==2.14.0)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/d1/da/4f264c196325bb6e37a6285caec5b12a03def489b57cc1fdac02bb6272cd/tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.15,>=2.14.0 (from tensorflow-macos==2.14.0->tensorflow==2.14.0)\n",
      "  Obtaining dependency information for keras<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/fe/58/34d4d8f1aa11120c2d36d7ad27d0526164b1a8ae45990a2fede31d0e59bf/keras-2.14.0-py3-none-any.whl.metadata\n",
      "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.0)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/93/6d/66d48b03460768f523da62a57a7e14e5e95fdf339d79e996ce3cecda2cdb/fsspec-2024.3.1-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.35.0)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: fsspec in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.0) (2023.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from requests->transformers==4.35.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from requests->transformers==4.35.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from requests->transformers==4.35.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from requests->transformers==4.35.0) (2024.2.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.14.0->tensorflow==2.14.0) (0.38.4)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14.0)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/9e/8d/ddbcf81ec751d8ee5fd18ac11ff38a0e110f39dfbf105e6d9db69d556dd0/google_auth-2.29.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.29.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14.0)\n",
      "  Obtaining dependency information for google-auth-oauthlib<1.1,>=0.5 from https://files.pythonhosted.org/packages/4a/07/8d9a8186e6768b55dfffeb57c719bc03770cf8a970a074616ae6f9e26a57/google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14.0) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14.0)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/7a/13/e503968fefabd4c6b2650af21e110aa8466fe21432cd7c43a84577a89438/tensorboard_data_server-0.7.2-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14.0) (2.2.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14.0)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/fb/2b/a64c2d25a37aeb921fddb929111413049fc5f8b9a4c1aefaffaafe768d54/cachetools-5.3.3-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14.0) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14.0)\n",
      "  Obtaining dependency information for rsa<5,>=3.1.4 from https://files.pythonhosted.org/packages/49/97/fa78e3d2f65c02c8e1268b9aba606569fe97f6c8f7c2d74394553347c145/rsa-4.9-py3-none-any.whl.metadata\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14.0)\n",
      "  Obtaining dependency information for requests-oauthlib>=0.7.0 from https://files.pythonhosted.org/packages/3b/5d/63d4ae3b9daea098d5d6f5da83984853c1bbacd5dc826764b249fe119d24/requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14.0) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/durgesh/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14.0) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow==2.14.0)\n",
      "  Obtaining dependency information for oauthlib>=3.0.0 from https://files.pythonhosted.org/packages/7e/80/cab10959dc1faead58dc8384a781dfbf93cb4d33d50988f7a69f1b7c9bbe/oauthlib-3.2.2-py3-none-any.whl.metadata\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.14.0-cp311-cp311-macosx_12_0_arm64.whl (2.1 kB)\n",
      "Downloading tensorflow_macos-2.14.0-cp311-cp311-macosx_12_0_arm64.whl (199.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.2.0-cp311-cp311-macosx_10_9_universal2.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.14.1-cp311-cp311-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.7-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.62.1-cp311-cp311-macosx_10_10_universal2.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-macosx_11_0_arm64.whl (26.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-macosx_12_0_arm64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.2/189.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, opt-einsum, oauthlib, ml-dtypes, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, huggingface-hub, google-auth, tokenizers, google-auth-oauthlib, transformers, tensorboard, tensorflow-macos, tensorflow\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.15.1\n",
      "    Uninstalling huggingface-hub-0.15.1:\n",
      "      Successfully uninstalled huggingface-hub-0.15.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.2\n",
      "    Uninstalling tokenizers-0.13.2:\n",
      "      Successfully uninstalled tokenizers-0.13.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.32.1\n",
      "    Uninstalling transformers-4.32.1:\n",
      "      Successfully uninstalled transformers-4.32.1\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.3 flatbuffers-24.3.7 gast-0.5.4 google-auth-2.29.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.62.1 huggingface-hub-0.17.3 keras-2.14.0 libclang-18.1.1 ml-dtypes-0.2.0 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.25.3 requests-oauthlib-2.0.0 rsa-4.9 tensorboard-2.14.1 tensorboard-data-server-0.7.2 tensorflow-2.14.0 tensorflow-estimator-2.14.0 tensorflow-io-gcs-filesystem-0.36.0 tensorflow-macos-2.14.0 termcolor-2.4.0 tokenizers-0.14.1 transformers-4.35.0\n"
     ]
    }
   ],
   "source": [
    "# make sure that the correct version of transformers and tensorflow is installed\n",
    "# https://huggingface.co/docs/transformers/index\n",
    "# https://www.tensorflow.org/install/pip\n",
    "!pip install transformers==4.35.0 tensorflow==2.14.0\n",
    "\n",
    "# if you are on a mac w/ an M1 chip, you'll need a specific tensorflow-metal version\n",
    "# !pip install tensorflow-metal==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "70O-tnuj3U72"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if you want, not required\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPgW_W1i3U73"
   },
   "source": [
    "Ensure that your versions match the ones below. If you are running this locally, install the packages with :\n",
    "\n",
    "```bash\n",
    "pip install transformers==4.35.0 tensorflow==2.14.0\n",
    "```\n",
    "\n",
    "If you get an error on import like:\n",
    "\n",
    "```\n",
    "NotFoundError: dlopen(ENVIRONMENT PATH/lib/python3.10/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): symbol not found in flat namespace '__ZN10tensorflow8internal10LogMessage16VmoduleActivatedEPKci'\n",
    "```\n",
    "\n",
    "This means that you are on a mac and you don't have the right `tensorflow-metal` [version installed](https://pypi.org/project/tensorflow-metal/):\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install tensorflow-metal==1.1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dc5ROhEU3U73"
   },
   "source": [
    "Check your versions with the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BzyFio-l3U73",
    "outputId": "d263f3e3-4e5b-4f1a-f328-2234296c29ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensforflow Version :  2.14.0\n",
      "Transformers Version :  4.35.0\n"
     ]
    }
   ],
   "source": [
    "# Version Info\n",
    "print(\"Tensforflow Version : \" ,tf.__version__)\n",
    "print(\"Transformers Version : \" ,transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPd9hZXtC1RB"
   },
   "source": [
    "## TASK 2: The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3R6TcHg3U74"
   },
   "source": [
    "We will be using reviews from the UCI Machine Learning Repository\n",
    "\n",
    "URL : https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
    "\n",
    "This should have three files :\n",
    "\n",
    "1. imbd_labelled.txt\n",
    "2. amazon_cells_labelled.txt\n",
    "3. yelp_labelled.txt\n",
    "\n",
    "We use the tensorflow utils to download and extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYulutFq3U74",
    "outputId": "845fb976-5aff-4f98-f200-118dea518ef5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://archive.ics.uci.edu/static/public/331/sentiment+labelled+sentences.zip\n",
      "   8192/Unknown - 0s 0us/step\n",
      " data/datasets/sentiment+labelled+sentences.zip\n"
     ]
    }
   ],
   "source": [
    "# make a data folder\n",
    "!mkdir -p data\n",
    "\n",
    "# download the data\n",
    "out_path = tf.keras.utils.get_file(origin=\"https://archive.ics.uci.edu/static/public/331/sentiment+labelled+sentences.zip\",extract=True,cache_dir=\"data\")\n",
    "print(\"\\n\",out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUrzd6_x3U74"
   },
   "source": [
    "Let's take a peek at the data with `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "X9vOov4r3U74",
    "outputId": "5d5f1162-1126-44bc-ce1f-14237cc10596"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>I just got bored watching Jessice Lange take h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>Unfortunately, any virtue in this film's produ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>In a word, it is embarrassing.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>Exceptionally bad!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>All in all its an insult to one's intelligence...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>748 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  1\n",
       "0    A very, very, very slow-moving, aimless movie ...  0\n",
       "1    Not sure who was more lost - the flat characte...  0\n",
       "2    Attempting artiness with black & white and cle...  0\n",
       "3         Very little music or anything to speak of.    0\n",
       "4    The best scene in the movie was when Gerardo i...  1\n",
       "..                                                 ... ..\n",
       "743  I just got bored watching Jessice Lange take h...  0\n",
       "744  Unfortunately, any virtue in this film's produ...  0\n",
       "745                   In a word, it is embarrassing.    0\n",
       "746                               Exceptionally bad!    0\n",
       "747  All in all its an insult to one's intelligence...  0\n",
       "\n",
       "[748 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewing the data\n",
    "imdb_data = \"data/datasets/sentiment labelled sentences/imdb_labelled.txt\"\n",
    "imdb_df = pd.read_csv(imdb_data, sep=\"\\t\", header=None)\n",
    "imdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o12HL7Cs3U75"
   },
   "source": [
    "Models usually take batches of the same size. It would be useful to check the distribution of the lengths of the sentences in the dataset. (We will need to define a max length of sentences to use.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "Emrbxhyk3U75",
    "outputId": "93e32eae-102e-4f1a-b1e8-e84cbb0f1666"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGgCAYAAACABpytAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAoLElEQVR4nO3df3DU1b3/8deaH0sSky1JYJetAaJN648ESoM3Er2SNiFcLsh1uFNUEHGgMyAQ2QIXiNwZcjuaIHcKtMMtHbgMKFwmnTuCl15/EW4xyo1UjHIloRfpECFo1mgNm6Bxg+F8//Dy+boExIVgTtbnY+bMuOe8d/e8E3Vfc3Y/G5cxxggAAMAi1/X1BgAAAC5EQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1okqoAwfPlwul6vHmD9/viTJGKOKigr5/X4lJSWpqKhIjY2NEY8RDodVVlamzMxMpaSkaPLkyTp16lTvdQQAAPo9VzR/i+fDDz9Ud3e3c7uhoUHjxo3Tvn37VFRUpCeffFJPPPGEtm7dqu9///t6/PHH9corr+jo0aNKTU2VJD3yyCP6/e9/r61btyojI0OLFy/Wxx9/rPr6esXFxX2tfZw7d07vv/++UlNT5XK5omwZAAD0BWOMOjo65Pf7dd11lzkjMVdh4cKF5qabbjLnzp0z586dMz6fz6xatcpZ/+yzz4zH4zG//e1vjTHGnD592iQkJJjq6mqn5r333jPXXXedefHFF7/28zY3NxtJDAaDwWAw+uFobm6+7Gt9vK5QV1eXtm/frkWLFsnlcun48eMKBoMqLS11atxut8aOHau6ujrNmTNH9fX1Onv2bESN3+9Xbm6u6urqNH78+Is+VzgcVjgcdm6b/zv0aW5uVlpa2pW2AAAAvkHt7e3Kyspy3lX5KlccUJ599lmdPn1aDz/8sCQpGAxKkrxeb0Sd1+vViRMnnJrExEQNHDiwR835+19MVVWV/umf/qnHfFpaGgEFAIB+5ut8POOKr+LZvHmzJkyYIL/f/5VPaoy57EYuV1NeXq5QKOSM5ubmK902AADoB64ooJw4cUJ79+7Vz372M2fO5/NJUo+TkNbWVudUxefzqaurS21tbZesuRi32+2clnBqAgBA7LuigLJlyxYNHjxYEydOdOays7Pl8/lUU1PjzHV1dam2tlaFhYWSpPz8fCUkJETUtLS0qKGhwakBAACI+jMo586d05YtWzRz5kzFx///u7tcLgUCAVVWVionJ0c5OTmqrKxUcnKypk2bJknyeDyaPXu2Fi9erIyMDKWnp2vJkiXKy8tTSUlJ73UFAAD6tagDyt69e3Xy5EnNmjWrx9rSpUvV2dmpefPmqa2tTQUFBdqzZ0/Ep3XXrl2r+Ph4TZ06VZ2dnSouLtbWrVu/9negAACA2BfVF7XZor29XR6PR6FQiM+jAADQT0Tz+s3f4gEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArBP1N8l+Gwxf/txla95dNfGyNQAA4MpwggIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArBN1QHnvvff04IMPKiMjQ8nJyfrhD3+o+vp6Z90Yo4qKCvn9fiUlJamoqEiNjY0RjxEOh1VWVqbMzEylpKRo8uTJOnXq1NV3AwAAYkJUAaWtrU133nmnEhIS9MILL+jIkSP65S9/qe985ztOzerVq7VmzRqtX79eBw8elM/n07hx49TR0eHUBAIB7dq1S9XV1dq/f7/OnDmjSZMmqbu7u9caAwAA/ZfLGGO+bvHy5cv13//933r11Vcvum6Mkd/vVyAQ0LJlyyR9cVri9Xr15JNPas6cOQqFQho0aJC2bdum++67T5L0/vvvKysrS88//7zGjx/f43HD4bDC4bBzu729XVlZWQqFQkpLS4uq4a9j+PLnLlvz7qqJvf68AADEsvb2dnk8nq/1+h3VCcru3bs1evRo/fSnP9XgwYM1atQobdq0yVlvampSMBhUaWmpM+d2uzV27FjV1dVJkurr63X27NmIGr/fr9zcXKfmQlVVVfJ4PM7IysqKZtsAAKCfiSqgHD9+XBs2bFBOTo5eeuklzZ07V48++qiefvppSVIwGJQkeb3eiPt5vV5nLRgMKjExUQMHDrxkzYXKy8sVCoWc0dzcHM22AQBAPxMfTfG5c+c0evRoVVZWSpJGjRqlxsZGbdiwQQ899JBT53K5Iu5njOkxd6GvqnG73XK73dFsFQAA9GNRnaAMGTJEt956a8TcLbfcopMnT0qSfD6fJPU4CWltbXVOVXw+n7q6utTW1nbJGgAA8O0WVUC58847dfTo0Yi5d955R8OGDZMkZWdny+fzqaamxlnv6upSbW2tCgsLJUn5+flKSEiIqGlpaVFDQ4NTAwAAvt2ieovn5z//uQoLC1VZWampU6fq9ddf18aNG7Vx40ZJX7y1EwgEVFlZqZycHOXk5KiyslLJycmaNm2aJMnj8Wj27NlavHixMjIylJ6eriVLligvL08lJSW93yEAAOh3ogoot99+u3bt2qXy8nL94he/UHZ2ttatW6fp06c7NUuXLlVnZ6fmzZuntrY2FRQUaM+ePUpNTXVq1q5dq/j4eE2dOlWdnZ0qLi7W1q1bFRcX13udAQCAfiuq70GxRTTXUV8JvgcFAIDed82+BwUAAOCbQEABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArBNVQKmoqJDL5YoYPp/PWTfGqKKiQn6/X0lJSSoqKlJjY2PEY4TDYZWVlSkzM1MpKSmaPHmyTp061TvdAACAmBD1Ccptt92mlpYWZxw+fNhZW716tdasWaP169fr4MGD8vl8GjdunDo6OpyaQCCgXbt2qbq6Wvv379eZM2c0adIkdXd3905HAACg34uP+g7x8RGnJucZY7Ru3TqtWLFCU6ZMkSQ99dRT8nq92rFjh+bMmaNQKKTNmzdr27ZtKikpkSRt375dWVlZ2rt3r8aPH3+V7QAAgFgQ9QnKsWPH5Pf7lZ2drfvvv1/Hjx+XJDU1NSkYDKq0tNSpdbvdGjt2rOrq6iRJ9fX1Onv2bESN3+9Xbm6uU3Mx4XBY7e3tEQMAAMSuqAJKQUGBnn76ab300kvatGmTgsGgCgsL9Ze//EXBYFCS5PV6I+7j9XqdtWAwqMTERA0cOPCSNRdTVVUlj8fjjKysrGi2DQAA+pmoAsqECRP093//98rLy1NJSYmee+45SV+8lXOey+WKuI8xpsfchS5XU15erlAo5Izm5uZotg0AAPqZq7rMOCUlRXl5eTp27JjzuZQLT0JaW1udUxWfz6euri61tbVdsuZi3G630tLSIgYAAIhdVxVQwuGw/vSnP2nIkCHKzs6Wz+dTTU2Ns97V1aXa2loVFhZKkvLz85WQkBBR09LSooaGBqcGAAAgqqt4lixZonvuuUdDhw5Va2urHn/8cbW3t2vmzJlyuVwKBAKqrKxUTk6OcnJyVFlZqeTkZE2bNk2S5PF4NHv2bC1evFgZGRlKT0/XkiVLnLeMAAAApCgDyqlTp/TAAw/oo48+0qBBg3THHXfowIEDGjZsmCRp6dKl6uzs1Lx589TW1qaCggLt2bNHqampzmOsXbtW8fHxmjp1qjo7O1VcXKytW7cqLi6udzsDAAD9lssYY/p6E9Fqb2+Xx+NRKBS6Jp9HGb78ucvWvLtqYq8/LwAAsSya12/+Fg8AALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABY56oCSlVVlVwulwKBgDNnjFFFRYX8fr+SkpJUVFSkxsbGiPuFw2GVlZUpMzNTKSkpmjx5sk6dOnU1WwEAADHkigPKwYMHtXHjRo0YMSJifvXq1VqzZo3Wr1+vgwcPyufzady4cero6HBqAoGAdu3aperqau3fv19nzpzRpEmT1N3dfeWdAACAmHFFAeXMmTOaPn26Nm3apIEDBzrzxhitW7dOK1as0JQpU5Sbm6unnnpKn376qXbs2CFJCoVC2rx5s375y1+qpKREo0aN0vbt23X48GHt3bv3os8XDofV3t4eMQAAQOy6ooAyf/58TZw4USUlJRHzTU1NCgaDKi0tdebcbrfGjh2ruro6SVJ9fb3Onj0bUeP3+5Wbm+vUXKiqqkoej8cZWVlZV7JtAADQT0QdUKqrq/Xmm2+qqqqqx1owGJQkeb3eiHmv1+usBYNBJSYmRpy8XFhzofLycoVCIWc0NzdHu20AANCPxEdT3NzcrIULF2rPnj0aMGDAJetcLlfEbWNMj7kLfVWN2+2W2+2OZqsAAKAfi+oEpb6+Xq2trcrPz1d8fLzi4+NVW1urX//614qPj3dOTi48CWltbXXWfD6furq61NbWdskaAADw7RZVQCkuLtbhw4d16NAhZ4wePVrTp0/XoUOHdOONN8rn86mmpsa5T1dXl2pra1VYWChJys/PV0JCQkRNS0uLGhoanBoAAPDtFtVbPKmpqcrNzY2YS0lJUUZGhjMfCARUWVmpnJwc5eTkqLKyUsnJyZo2bZokyePxaPbs2Vq8eLEyMjKUnp6uJUuWKC8vr8eHbgEAwLdTVAHl61i6dKk6Ozs1b948tbW1qaCgQHv27FFqaqpTs3btWsXHx2vq1Knq7OxUcXGxtm7dqri4uN7eDgAA6IdcxhjT15uIVnt7uzwej0KhkNLS0nr98Ycvf+6yNe+umtjrzwsAQCyL5vWbv8UDAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArBNVQNmwYYNGjBihtLQ0paWlacyYMXrhhRecdWOMKioq5Pf7lZSUpKKiIjU2NkY8RjgcVllZmTIzM5WSkqLJkyfr1KlTvdMNAACICVEFlBtuuEGrVq3SG2+8oTfeeEM/+clP9Hd/93dOCFm9erXWrFmj9evX6+DBg/L5fBo3bpw6OjqcxwgEAtq1a5eqq6u1f/9+nTlzRpMmTVJ3d3fvdgYAAPotlzHGXM0DpKen65//+Z81a9Ys+f1+BQIBLVu2TNIXpyVer1dPPvmk5syZo1AopEGDBmnbtm267777JEnvv/++srKy9Pzzz2v8+PFf6znb29vl8XgUCoWUlpZ2Ndu/qOHLn7tszburJvb68wIAEMuief2+4s+gdHd3q7q6Wp988onGjBmjpqYmBYNBlZaWOjVut1tjx45VXV2dJKm+vl5nz56NqPH7/crNzXVqLiYcDqu9vT1iAACA2BV1QDl8+LCuv/56ud1uzZ07V7t27dKtt96qYDAoSfJ6vRH1Xq/XWQsGg0pMTNTAgQMvWXMxVVVV8ng8zsjKyop22wAAoB+JOqD84Ac/0KFDh3TgwAE98sgjmjlzpo4cOeKsu1yuiHpjTI+5C12upry8XKFQyBnNzc3RbhsAAPQjUQeUxMREfe9739Po0aNVVVWlkSNH6le/+pV8Pp8k9TgJaW1tdU5VfD6furq61NbWdsmai3G73c6VQ+cHAACIXVf9PSjGGIXDYWVnZ8vn86mmpsZZ6+rqUm1trQoLCyVJ+fn5SkhIiKhpaWlRQ0ODUwMAABAfTfFjjz2mCRMmKCsrSx0dHaqurtbLL7+sF198US6XS4FAQJWVlcrJyVFOTo4qKyuVnJysadOmSZI8Ho9mz56txYsXKyMjQ+np6VqyZIny8vJUUlJyTRoEAAD9T1QB5YMPPtCMGTPU0tIij8ejESNG6MUXX9S4ceMkSUuXLlVnZ6fmzZuntrY2FRQUaM+ePUpNTXUeY+3atYqPj9fUqVPV2dmp4uJibd26VXFxcb3bGQAA6Leu+ntQ+gLfgwIAQP/zjXwPCgAAwLVCQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWiSqgVFVV6fbbb1dqaqoGDx6se++9V0ePHo2oMcaooqJCfr9fSUlJKioqUmNjY0RNOBxWWVmZMjMzlZKSosmTJ+vUqVNX3w0AAIgJUQWU2tpazZ8/XwcOHFBNTY0+//xzlZaW6pNPPnFqVq9erTVr1mj9+vU6ePCgfD6fxo0bp46ODqcmEAho165dqq6u1v79+3XmzBlNmjRJ3d3dvdcZAADot1zGGHOld/7www81ePBg1dbW6u6775YxRn6/X4FAQMuWLZP0xWmJ1+vVk08+qTlz5igUCmnQoEHatm2b7rvvPknS+++/r6ysLD3//PMaP358j+cJh8MKh8PO7fb2dmVlZSkUCiktLe1Kt39Jw5c/d9mad1dN7PXnBQAglrW3t8vj8Xyt1++r+gxKKBSSJKWnp0uSmpqaFAwGVVpa6tS43W6NHTtWdXV1kqT6+nqdPXs2osbv9ys3N9epuVBVVZU8Ho8zsrKyrmbbAADAclccUIwxWrRoke666y7l5uZKkoLBoCTJ6/VG1Hq9XmctGAwqMTFRAwcOvGTNhcrLyxUKhZzR3Nx8pdsGAAD9QPyV3nHBggV6++23tX///h5rLpcr4rYxpsfchb6qxu12y+12X+lWAQBAP3NFJyhlZWXavXu39u3bpxtuuMGZ9/l8ktTjJKS1tdU5VfH5fOrq6lJbW9slawAAwLdbVAHFGKMFCxZo586d+sMf/qDs7OyI9ezsbPl8PtXU1DhzXV1dqq2tVWFhoSQpPz9fCQkJETUtLS1qaGhwagAAwLdbVG/xzJ8/Xzt27NB//Md/KDU11Tkp8Xg8SkpKksvlUiAQUGVlpXJycpSTk6PKykolJydr2rRpTu3s2bO1ePFiZWRkKD09XUuWLFFeXp5KSkp6v0MAANDvRBVQNmzYIEkqKiqKmN+yZYsefvhhSdLSpUvV2dmpefPmqa2tTQUFBdqzZ49SU1Od+rVr1yo+Pl5Tp05VZ2eniouLtXXrVsXFxV1dNwAAICZc1feg9JVorqO+EnwPCgAAve8b+x4UAACAa4GAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFgn6oDyyiuv6J577pHf75fL5dKzzz4bsW6MUUVFhfx+v5KSklRUVKTGxsaImnA4rLKyMmVmZiolJUWTJ0/WqVOnrqoRAAAQO6IOKJ988olGjhyp9evXX3R99erVWrNmjdavX6+DBw/K5/Np3Lhx6ujocGoCgYB27dql6upq7d+/X2fOnNGkSZPU3d195Z0AAICYER/tHSZMmKAJEyZcdM0Yo3Xr1mnFihWaMmWKJOmpp56S1+vVjh07NGfOHIVCIW3evFnbtm1TSUmJJGn79u3KysrS3r17NX78+KtoBwAAxIJe/QxKU1OTgsGgSktLnTm3262xY8eqrq5OklRfX6+zZ89G1Pj9fuXm5jo1FwqHw2pvb48YAAAgdvVqQAkGg5Ikr9cbMe/1ep21YDCoxMREDRw48JI1F6qqqpLH43FGVlZWb24bAABY5ppcxeNyuSJuG2N6zF3oq2rKy8sVCoWc0dzc3Gt7BQAA9unVgOLz+SSpx0lIa2urc6ri8/nU1dWltra2S9ZcyO12Ky0tLWIAAIDY1asBJTs7Wz6fTzU1Nc5cV1eXamtrVVhYKEnKz89XQkJCRE1LS4saGhqcGgAA8O0W9VU8Z86c0Z///GfndlNTkw4dOqT09HQNHTpUgUBAlZWVysnJUU5OjiorK5WcnKxp06ZJkjwej2bPnq3FixcrIyND6enpWrJkifLy8pyregAAwLdb1AHljTfe0I9//GPn9qJFiyRJM2fO1NatW7V06VJ1dnZq3rx5amtrU0FBgfbs2aPU1FTnPmvXrlV8fLymTp2qzs5OFRcXa+vWrYqLi+uFlgAAQH/nMsaYvt5EtNrb2+XxeBQKha7J51GGL3/usjXvrprY688LAEAsi+b1m7/FAwAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsE/VX3eMLfNssAADXDicoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1+jSg/OY3v1F2drYGDBig/Px8vfrqq325HQAAYIk+Cyi/+93vFAgEtGLFCr311lv667/+a02YMEEnT57sqy0BAABLuIwxpi+euKCgQD/60Y+0YcMGZ+6WW27Rvffeq6qqqojacDiscDjs3A6FQho6dKiam5uVlpbW63vLXflSrz/mpTT80/hv7LkAAOhL7e3tysrK0unTp+XxeL662PSBcDhs4uLizM6dOyPmH330UXP33Xf3qF+5cqWRxGAwGAwGIwZGc3PzZbNCvPrARx99pO7ubnm93oh5r9erYDDYo768vFyLFi1ybp87d04ff/yxMjIy5HK5em1f55PdtTqZsQE9xgZ6jA2x3mOs9yfRY7SMMero6JDf779sbZ8ElPMuDBfGmIsGDrfbLbfbHTH3ne9855rtKy0tLWb/RTuPHmMDPcaGWO8x1vuT6DEal31r5//0yYdkMzMzFRcX1+O0pLW1tcepCgAA+Pbpk4CSmJio/Px81dTURMzX1NSosLCwL7YEAAAs0mdv8SxatEgzZszQ6NGjNWbMGG3cuFEnT57U3Llz+2pLcrvdWrlyZY+3k2IJPcYGeowNsd5jrPcn0eO11GeXGUtffFHb6tWr1dLSotzcXK1du1Z33313X20HAABYok8DCgAAwMXwt3gAAIB1CCgAAMA6BBQAAGAdAgoAALAOAeVLfvOb3yg7O1sDBgxQfn6+Xn311b7e0kW98soruueee+T3++VyufTss89GrBtjVFFRIb/fr6SkJBUVFamxsTGiJhwOq6ysTJmZmUpJSdHkyZN16tSpiJq2tjbNmDFDHo9HHo9HM2bM0OnTp69xd1JVVZVuv/12paamavDgwbr33nt19OjRiJr+3uOGDRs0YsQI55sZx4wZoxdeeCFm+ruYqqoquVwuBQIBZ66/91lRUSGXyxUxfD5fzPR33nvvvacHH3xQGRkZSk5O1g9/+EPV19c76/29z+HDh/f4PbpcLs2fPz8m+vv888/1j//4j8rOzlZSUpJuvPFG/eIXv9C5c+ecGit7vIq/+RdTqqurTUJCgtm0aZM5cuSIWbhwoUlJSTEnTpzo66318Pzzz5sVK1aYZ555xkgyu3btilhftWqVSU1NNc8884w5fPiwue+++8yQIUNMe3u7UzN37lzz3e9+19TU1Jg333zT/PjHPzYjR440n3/+uVPzN3/zNyY3N9fU1dWZuro6k5ubayZNmnTN+xs/frzZsmWLaWhoMIcOHTITJ040Q4cONWfOnImZHnfv3m2ee+45c/ToUXP06FHz2GOPmYSEBNPQ0BAT/V3o9ddfN8OHDzcjRowwCxcudOb7e58rV640t912m2lpaXFGa2trzPRnjDEff/yxGTZsmHn44YfNH//4R9PU1GT27t1r/vznP8dMn62trRG/w5qaGiPJ7Nu3Lyb6e/zxx01GRob5z//8T9PU1GT+/d//3Vx//fVm3bp1To2NPRJQ/s9f/dVfmblz50bM3XzzzWb58uV9tKOv58KAcu7cOePz+cyqVaucuc8++8x4PB7z29/+1hhjzOnTp01CQoKprq52at577z1z3XXXmRdffNEYY8yRI0eMJHPgwAGn5rXXXjOSzP/+7/9e464itba2GkmmtrbWGBObPRpjzMCBA82//uu/xlx/HR0dJicnx9TU1JixY8c6ASUW+ly5cqUZOXLkRddioT9jjFm2bJm56667LrkeK31+2cKFC81NN91kzp07FxP9TZw40cyaNStibsqUKebBBx80xtj7O+QtHkldXV2qr69XaWlpxHxpaanq6ur6aFdXpqmpScFgMKIXt9utsWPHOr3U19fr7NmzETV+v1+5ublOzWuvvSaPx6OCggKn5o477pDH4/nGfyahUEiSlJ6eLin2euzu7lZ1dbU++eQTjRkzJub6mz9/viZOnKiSkpKI+Vjp89ixY/L7/crOztb999+v48ePx1R/u3fv1ujRo/XTn/5UgwcP1qhRo7Rp0yZnPVb6PK+rq0vbt2/XrFmz5HK5YqK/u+66S//1X/+ld955R5L0P//zP9q/f7/+9m//VpK9v8M+/WvGtvjoo4/U3d3d4w8Ver3eHn/Q0Hbn93uxXk6cOOHUJCYmauDAgT1qzt8/GAxq8ODBPR5/8ODB3+jPxBijRYsW6a677lJubq6zt/P7/bL+1uPhw4c1ZswYffbZZ7r++uu1a9cu3Xrrrc5/yP29P0mqrq7Wm2++qYMHD/ZYi4XfY0FBgZ5++ml9//vf1wcffKDHH39chYWFamxsjIn+JOn48ePasGGDFi1apMcee0yvv/66Hn30Ubndbj300EMx0+d5zz77rE6fPq2HH37Y2df5vX5Zf+pv2bJlCoVCuvnmmxUXF6fu7m498cQTeuCBB5y9nd/vhfvvyx4JKF/icrkibhtjesz1F1fSy4U1F6v/pn8mCxYs0Ntvv639+/f3WOvvPf7gBz/QoUOHdPr0aT3zzDOaOXOmamtrL7m3/tZfc3OzFi5cqD179mjAgAGXrOvPfU6YMMH557y8PI0ZM0Y33XSTnnrqKd1xxx0X3Vt/6k+Szp07p9GjR6uyslKSNGrUKDU2NmrDhg166KGHLrnH/tbneZs3b9aECRPk9/sj5vtzf7/73e+0fft27dixQ7fddpsOHTqkQCAgv9+vmTNnXnJ/fd0jb/FIyszMVFxcXI+E19ra2iNR2u78FQRf1YvP51NXV5fa2tq+suaDDz7o8fgffvjhN/YzKSsr0+7du7Vv3z7dcMMNznys9JiYmKjvfe97Gj16tKqqqjRy5Ej96le/ipn+6uvr1draqvz8fMXHxys+Pl61tbX69a9/rfj4eGcP/b3PL0tJSVFeXp6OHTsWM7/HIUOG6NZbb42Yu+WWW3Ty5Elnf1L/71OSTpw4ob179+pnP/uZMxcL/f3DP/yDli9frvvvv195eXmaMWOGfv7zn6uqqsrZm2RfjwQUffFCkZ+fr5qamoj5mpoaFRYW9tGurkx2drZ8Pl9EL11dXaqtrXV6yc/PV0JCQkRNS0uLGhoanJoxY8YoFArp9ddfd2r++Mc/KhQKXfOfiTFGCxYs0M6dO/WHP/xB2dnZEeux0OPFGGMUDodjpr/i4mIdPnxYhw4dcsbo0aM1ffp0HTp0SDfeeGNM9Pll4XBYf/rTnzRkyJCY+T3eeeedPS7zf+eddzRs2DBJsfXf45YtWzR48GBNnDjRmYuF/j799FNdd13ky31cXJxzmbG1PUb9sdoYdf4y482bN5sjR46YQCBgUlJSzLvvvtvXW+uho6PDvPXWW+att94yksyaNWvMW2+95VwSvWrVKuPxeMzOnTvN4cOHzQMPPHDRy8VuuOEGs3fvXvPmm2+an/zkJxe9XGzEiBHmtddeM6+99prJy8v7Ri6Je+SRR4zH4zEvv/xyxKV/n376qVPT33ssLy83r7zyimlqajJvv/22eeyxx8x1111n9uzZExP9XcqXr+Ixpv/3uXjxYvPyyy+b48ePmwMHDphJkyaZ1NRU5/8b/b0/Y764RDw+Pt488cQT5tixY+bf/u3fTHJystm+fbtTEwt9dnd3m6FDh5ply5b1WOvv/c2cOdN897vfdS4z3rlzp8nMzDRLly61ukcCypf8y7/8ixk2bJhJTEw0P/rRj5zLWm2zb98+I6nHmDlzpjHmi0vGVq5caXw+n3G73ebuu+82hw8fjniMzs5Os2DBApOenm6SkpLMpEmTzMmTJyNq/vKXv5jp06eb1NRUk5qaaqZPn27a2tqueX8X602S2bJli1PT33ucNWuW8+/aoEGDTHFxsRNOYqG/S7kwoPT3Ps9/V0RCQoLx+/1mypQpprGxMWb6O+/3v/+9yc3NNW6329x8881m48aNEeux0OdLL71kJJmjR4/2WOvv/bW3t5uFCxeaoUOHmgEDBpgbb7zRrFixwoTDYat7dBljTPTnLgAAANcOn0EBAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHX+HxRhvmghQFu7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: plot the length of the reviews\n",
    "# hint: use the .str.len() method on the first column of the dataframe to get the length of the sentences out\n",
    "# y axis should be the count/number of reviews\n",
    "# x axis should be the length of the reviews\n",
    "# a histogram is a good choice here for visualization\n",
    "# label your axes!\n",
    "plt.hist(imdb_df[0].str.len(),bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmigAhUuC4Yv"
   },
   "source": [
    "**TODO:** Explore the other two data files, similarly to what we have done above (and what we've done on our homeworks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMGFRmnWDBEu"
   },
   "outputs": [],
   "source": [
    "# Other data files explored here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrhH5gxhDGWN"
   },
   "source": [
    "## TASK 3: Setting BERT up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgyFw7zkDIM_"
   },
   "source": [
    "__IMPORTANT__:\n",
    "BERT uses two special tokens while tokenization, they are [CLS] and [SEP]. The [CLS] token is used to capture the overall representation of the input sequence for classification tasks, while the [SEP] token is used to indicate sentence boundaries and separate different segments of text, enabling BERT to understand relationships between sentences. These tokens play a vital role in enabling BERT to handle a wide range of natural language processing tasks effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cstsfT2B3U75"
   },
   "source": [
    "Let us now begin by understanding the Tokenizer used by BERT and the BERT Model Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "bdagkfGR3U76"
   },
   "outputs": [],
   "source": [
    "# import specific models\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# define a max length constant\n",
    "MAX_LENGTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A8NP_1_q3U76",
    "outputId": "f28430e9-4a8f-4851-ed36-366e427f5888"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a822f1dff1a74ed492a97e316cd7c204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f63108715a142bfbbf046ab2227e4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73f8808da1848df933fa126bc226d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688b4f15726f47f39f4f7e225246d7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4aacfdc93d428991c2debc16cf5332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# downloads a bunch of stuff. On Raj's computer, this took ~ 20 seconds\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# expect to see:\n",
    "# Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
    "# - This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
    "# - This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "# All the weights of TFBertModel were initialized from the PyTorch model.\n",
    "# If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dyC5gU3v-YyG",
    "outputId": "4a033b65-efcf-48ec-f8b4-d59f89b9eb51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109482240 (417.64 MB)\n",
      "Trainable params: 109482240 (417.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# TODO: print out a summary of your downloaded bert model\n",
    "bert.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCz4eg5x-bal"
   },
   "source": [
    "Notice how there are 109M parameters. These parameters have already been trained. We want to make sure these are not trained when we add on our own components later. (We'll \"freeze\" them.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04eD8Fa42cVy"
   },
   "source": [
    "1. What kind of text data do these weights encode? (Scour the internet and describe the contents of the data used to train BERT)\n",
    "__YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmIp4Rs7DVUE"
   },
   "source": [
    "## The Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ze5NIO3x3U76",
    "outputId": "d8c5309a-0dac-4c44-f711-645d6ddc1d78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length is :  30522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19219"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: print out the first 10 tokens in the tokenizer's vocabulary, the\n",
    "# last ten tokens, and ten tokens from somewhere in the middle of the vocabulary\n",
    "# print out the things necessary to answer the questions below\n",
    "\n",
    "# tokenizer.vocab will access the vocabulary\n",
    "# tokenizer.vocab.keys() will give you the words in the vocabulary. You can convert this to a list to index into it.\n",
    "# tokenizer.vocab[string] will give you the token id for the string\n",
    "# tokenizer.decode(id) will give you the string for the token id\n",
    "\n",
    "print(\"Vocab length is : \",len(tokenizer.vocab))\n",
    "tokenizer.vocab['sud']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the token associated with id 2897? __network__\n",
    "3. What is the token associated with id 102? __sep__\n",
    "4. What is the token associated with id 103? __MASK__\n",
    "5. Is your name in BERT's vocabulary (make sure to used the lowercase version, e.g. \"raj\")? If yes, what is it's id?  __19219__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ahvhFZSE3U76",
    "outputId": "191e1cad-639f-4d1f-8fad-87b061724857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids :  tf.Tensor([[  101  7592  1010  2026  3899  2003 10140  1010  2053  2428   102]], shape=(1, 11), dtype=int32)\n",
      "token_type_ids :  tf.Tensor([[0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 11), dtype=int32)\n",
      "attention_mask :  tf.Tensor([[1 1 1 1 1 1 1 1 1 1 1]], shape=(1, 11), dtype=int32)\n",
      "[CLS] hello, my dog is cute, no really [SEP]\n"
     ]
    }
   ],
   "source": [
    "# example of tokenizing a sentence with the bert tokenizer\n",
    "input_text = \"Hello, my dog is cute, no really\"\n",
    "out = tokenizer(input_text,return_tensors=\"tf\")\n",
    "for key in out:\n",
    "    print(key,\": \",out[key])\n",
    "\n",
    "# notice the differences between the input_text and what is printed out here\n",
    "print(tokenizer.decode(out[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7aibPRm3U76"
   },
   "source": [
    "Notice the different keys returned by the tokenizer. The model can take as input all three keys or just one of them the \"input_ids\".\n",
    "Since the model requires a fixed length input, we will need to pad the sequences to the same length. Let us pad and turn the sequences into length 15 as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9rQN3WHF3U77",
    "outputId": "6d87b813-4038-4284-a770-195a7d91f0a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids :  tf.Tensor([[  101  7592  1010  2026  3899  2003  2025 10140  1010  2053  2428   102]], shape=(1, 12), dtype=int32)\n",
      "token_type_ids :  tf.Tensor([[0 0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 12), dtype=int32)\n",
      "attention_mask :  tf.Tensor([[1 1 1 1 1 1 1 1 1 1 1 1]], shape=(1, 12), dtype=int32)\n",
      "[CLS] hello, my dog is not cute, no really [SEP]\n"
     ]
    }
   ],
   "source": [
    "# TODO: set the max_length and truncation parameters of the tokenizer\n",
    "# the behavior that you want to end up with is a tokenizer that produces tensors that \n",
    "# are never over 15. You are not providing paired input\n",
    "\n",
    "# Tokenizer documentation: https://huggingface.co/docs/transformers/main_classes/tokenizer\n",
    "\n",
    "input_text = \"Hello, my dog is not cute, no really\"\n",
    "\n",
    "# TODO: update this line\n",
    "out = tokenizer(input_text,return_tensors=\"tf\")\n",
    "\n",
    "\n",
    "for key in out:\n",
    "    print(key,\": \",out[key])\n",
    "print(tokenizer.decode(out[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rb7Jhkmh3U77"
   },
   "source": [
    "The tokenzier also returns multiple outputs for multiple sentences. This is useful for batching.\n",
    "\n",
    "NOTE : The max length is 512 for BERT. We will use 128 or 256 for this lab (our `MAX_LENGTH` parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VybL0aDS3U77",
    "outputId": "8175cd1b-c8fc-4522-dea0-bceeb1a62f4a"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# here's some code to look at the shapes of the tensors that have been output\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m out:\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(key,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m,batch_x[key]\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# TODO: make a list of sentences that you want to tokenize with at least 3 sentences that are padded to MAX_LENGTH\n",
    "# make sure that you can tokenize them all at once by looking at the shapes of the tensors that have been output\n",
    "batch_x = ['this is the first string','the second string','third sentence is this']\n",
    "out = tokenizer(input_text,return_tensors=\"tf\")\n",
    "# here's some code to look at the shapes of the tensors that have been output\n",
    "for key in out:\n",
    "    print(key,\": \",batch_x[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqoZW31r3U77",
    "outputId": "71082962-4f64-4dfd-d091-7c3a448637d0"
   },
   "outputs": [],
   "source": [
    "# run bert on the input_ids\n",
    "out = bert(batch_x[\"input_ids\"])\n",
    "\n",
    "# display what the keys are for us\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "auI7ybZq3U77",
    "outputId": "b7ae6ed0-5cbd-40fd-c49e-5f080c09eb82"
   },
   "outputs": [],
   "source": [
    "# take a look at the shape of the last_hidden_state\n",
    "out['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZIJGjHL3U77"
   },
   "source": [
    "The model returns two tensors of interest. One is the pooled output and the other is contextual hidden state over every token. We will use the hidden state associated with the first token [CLS] for classification. Additional strategies involve averaging the non padding states. The pooler output is a summary of the hidden states.\n",
    "\n",
    "5. Investigate the shape of `last_hidden_state`. What is the __meaning__ of each dimension? (why are each dimension numbered as they are/what do they correspond to? You may need to go investigate some documentation to fully answer this!) __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8-nEKeH13U77",
    "outputId": "21c36e0f-0e09-4591-e33b-d97b1a00772b"
   },
   "outputs": [],
   "source": [
    "# we can also run bert with unpacked inputs\n",
    "out = bert(**batch_x)\n",
    "\n",
    "# or with them as \"input_ids\" explicitly\n",
    "out = bert(input_ids=batch_x[\"input_ids\"])\n",
    "\n",
    "print(\"First token [CLS] :\")\n",
    "out[\"last_hidden_state\"][:,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDOKY_s6B8Ie"
   },
   "source": [
    "## Create a Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Op6JBjoM3U78"
   },
   "outputs": [],
   "source": [
    "# define a batch size for our experiments\n",
    "BATCH_SIZE = 4\n",
    "# define a percentage of the data to use for training\n",
    "SPLIT_PC = .80\n",
    "\n",
    "# TODO: caluculate the last index for the training data\n",
    "END = int(SPLIT_PC * len(imdb_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO: get the training data and testing data set up\n",
    "train_x = list(imdb_df[0][:END])\n",
    "test_x = list(imdb_df[0][END:])\n",
    "\n",
    "train_y = list(imdb_df[1][:END])\n",
    "test_y = list(imdb_df[1][END:])\n",
    "\n",
    "\n",
    "# TODO: add print statements to verify the sizes of your training/testing data are correct\n",
    "\n",
    "# train_x.shape, test_x.shape, train_y.shape, test_y.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBGgSLsK3U78",
    "outputId": "947b8caa-256c-4934-d11c-f3c5948fbc49"
   },
   "outputs": [],
   "source": [
    "# data generator for the model\n",
    "def data_generator(sentences: np.array,labels: np.array,batch_size: int) -> (dict,tf.Tensor):\n",
    "    i = 0\n",
    "    while True:\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        # TODO: append batch_size number of sentences and labels to batch_x and batch_y\n",
    "        # Make sure that you don't re-use sentences and labels that you've already put into batches!\n",
    "        for j in range(batch_size):\n",
    "#             print(sentences[i])\n",
    "            batch_x.append(sentences[i])\n",
    "            batch_y.append(labels[i])\n",
    "        \n",
    "            i = (i+1)%len(sentences)\n",
    "\n",
    "        # TODO: tokenize the batch_x, padding to MAX_LENGTH, and truncating to MAX_LENGTH\n",
    "        batch_x =tokenizer(batch_x,return_tensors=\"tf\", padding='max_length', max_length=MAX_LENGTH, truncation=True)\n",
    "\n",
    "        # debugging prints (make sure that these are commented out when you actually train your model)\n",
    "        # should be (batch_size, MAX_LENGTH)\n",
    "        print(batch_x['input_ids'].shape)\n",
    "\n",
    "        # convert our ys into the appropriate tensor\n",
    "        batch_y = tf.convert_to_tensor(batch_y)\n",
    "        \n",
    "        # debugging prints (make sure that these are commented out when you actually train your model)\n",
    "        # should be (batch_size,)\n",
    "        # print(batch_y.shape)\n",
    "        yield dict(batch_x), batch_y\n",
    "\n",
    "train_data = data_generator(train_x,train_y,BATCH_SIZE)\n",
    "test_data = data_generator(test_x,test_y,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xwi_HtRmCfFs"
   },
   "source": [
    "How would you implement randomness in the generator above. (HINT `numpy.random.choice` and its `replace` option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "QxuNw_DKBN4T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 256)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Take a look at the contents of tmp_batch_x and tmp_batch_y and report the shapes of the `input_ids` \n",
    "# and the y label tensor.\n",
    "# make sure that the shapes are what you expect them to be\n",
    "# (take a look at the comments in the data_generator code)\n",
    "\n",
    "tmp_batch_x,tmp_batch_y = next(train_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n78JGhebB4C_"
   },
   "source": [
    "## TASK 4: Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DQjpBh_-liQ"
   },
   "source": [
    "Remember how you used concatenated embeddings for your NN model for HW5. The model we design in this lab will take as input token_ids (batch_size, embedding_dim) , run it through a non-trainable bert, extract the 768 dim vector associated with the [CLS] token of each sentence in a batch, this 768 dim vector will play the role of our concatenated embeddings. The main take away is this : Any input size up to 512 will return a 768 dim vector we can use as an embedding for the entire sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9T-oAymn3U78",
    "outputId": "b8f8949d-232a-41d0-ae28-6994008f7e85"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "# This takes ~5-7 sec to run on Raj's computer\n",
    "\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased',output_attentions = False,return_dict=False)\n",
    "# we do not need attention outputs\n",
    "# we want to return tuples since they are easier to access\n",
    "\n",
    "bert_model.trainable = False\n",
    "# setting trainable to false ensures\n",
    "# we do not update its weights\n",
    "model_ = tf.keras.Sequential([\n",
    "    bert_model,\n",
    "    tf.keras.layers.Lambda(lambda x: x[0][:,0,:]), # https://keras.io/api/layers/core_layers/lambda/\n",
    "    tf.keras.layers.Dense(50,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model_.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What does the `Lambda` layer do? Why do we need it? (Read the documentation and investigate. Think carefully about what `x[0][:,0,:]` means. `x[0]` here is a numpy array.) __YOUR ANSWER HERE__\n",
    "2. What weights will you be training in this model? __YOUR ANSWER HERE__\n",
    "2. What weights will you __not__ be training in this model? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heoBiDyaBl72"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "We use `.fit` here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_d_b3IWY_ZZz"
   },
   "source": [
    "We will also be adding a validation data generator and validation steps. \n",
    "This will allow us to check accuracy on the test_data wile we train over each epoch.\n",
    "\n",
    "For this part, you'll be training in some different configurations and __recording your results__. (don't forget to write these down!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O1Seudgl3U79",
    "outputId": "029c9b07-d7b3-4e07-cb4b-68824b2c285f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 256)\n",
      "Epoch 1/2\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "  1/149 [..............................] - ETA: 9:03 - loss: 0.8265 - accuracy: 0.5000(4, 256)\n",
      "  2/149 [..............................] - ETA: 1:28 - loss: 0.8893 - accuracy: 0.3750(4, 256)\n",
      "  3/149 [..............................] - ETA: 1:28 - loss: 0.8242 - accuracy: 0.4167(4, 256)\n",
      "  4/149 [..............................] - ETA: 1:27 - loss: 0.7760 - accuracy: 0.5000(4, 256)\n",
      "  5/149 [>.............................] - ETA: 1:26 - loss: 0.6950 - accuracy: 0.6000(4, 256)\n",
      "  6/149 [>.............................] - ETA: 1:26 - loss: 0.6333 - accuracy: 0.6667(4, 256)\n",
      "  7/149 [>.............................] - ETA: 1:25 - loss: 0.6941 - accuracy: 0.6429(4, 256)\n",
      "  8/149 [>.............................] - ETA: 1:24 - loss: 0.7130 - accuracy: 0.6250(4, 256)\n",
      "  9/149 [>.............................] - ETA: 1:24 - loss: 0.6452 - accuracy: 0.6667(4, 256)\n",
      " 10/149 [=>............................] - ETA: 1:23 - loss: 0.8009 - accuracy: 0.6000(4, 256)\n",
      " 11/149 [=>............................] - ETA: 1:23 - loss: 0.8485 - accuracy: 0.5682(4, 256)\n",
      " 12/149 [=>............................] - ETA: 1:22 - loss: 0.8693 - accuracy: 0.5625(4, 256)\n",
      " 13/149 [=>............................] - ETA: 1:21 - loss: 0.9281 - accuracy: 0.5192(4, 256)\n",
      " 14/149 [=>............................] - ETA: 1:21 - loss: 0.9077 - accuracy: 0.5357(4, 256)\n",
      " 15/149 [==>...........................] - ETA: 1:20 - loss: 0.8671 - accuracy: 0.5667(4, 256)\n",
      " 16/149 [==>...........................] - ETA: 1:19 - loss: 0.8447 - accuracy: 0.5781(4, 256)\n",
      " 17/149 [==>...........................] - ETA: 1:19 - loss: 0.8541 - accuracy: 0.5441(4, 256)\n",
      " 18/149 [==>...........................] - ETA: 1:18 - loss: 0.8580 - accuracy: 0.5139(4, 256)\n",
      " 19/149 [==>...........................] - ETA: 1:18 - loss: 0.8568 - accuracy: 0.5000(4, 256)\n",
      " 20/149 [===>..........................] - ETA: 1:17 - loss: 0.8491 - accuracy: 0.5000(4, 256)\n",
      " 21/149 [===>..........................] - ETA: 1:17 - loss: 0.8351 - accuracy: 0.5119(4, 256)\n",
      " 22/149 [===>..........................] - ETA: 1:16 - loss: 0.8233 - accuracy: 0.5114(4, 256)\n",
      " 23/149 [===>..........................] - ETA: 1:16 - loss: 0.8042 - accuracy: 0.5326(4, 256)\n",
      " 24/149 [===>..........................] - ETA: 1:16 - loss: 0.7831 - accuracy: 0.5521(4, 256)\n",
      " 25/149 [====>.........................] - ETA: 1:15 - loss: 0.7595 - accuracy: 0.5700(4, 256)\n",
      " 26/149 [====>.........................] - ETA: 1:14 - loss: 0.7562 - accuracy: 0.5769(4, 256)\n",
      " 27/149 [====>.........................] - ETA: 1:14 - loss: 0.7327 - accuracy: 0.5926(4, 256)\n",
      " 28/149 [====>.........................] - ETA: 1:13 - loss: 0.7096 - accuracy: 0.6071(4, 256)\n",
      " 29/149 [====>.........................] - ETA: 1:12 - loss: 0.6882 - accuracy: 0.6207(4, 256)\n",
      " 30/149 [=====>........................] - ETA: 1:12 - loss: 0.6671 - accuracy: 0.6333(4, 256)\n",
      " 31/149 [=====>........................] - ETA: 1:11 - loss: 0.6471 - accuracy: 0.6452(4, 256)\n",
      " 32/149 [=====>........................] - ETA: 1:11 - loss: 0.6915 - accuracy: 0.6328(4, 256)\n",
      " 33/149 [=====>........................] - ETA: 1:10 - loss: 0.7002 - accuracy: 0.6364(4, 256)\n",
      " 34/149 [=====>........................] - ETA: 1:09 - loss: 0.7015 - accuracy: 0.6397(4, 256)\n",
      " 35/149 [======>.......................] - ETA: 1:09 - loss: 0.7684 - accuracy: 0.6214(4, 256)\n",
      " 36/149 [======>.......................] - ETA: 1:08 - loss: 0.8244 - accuracy: 0.6042(4, 256)\n",
      " 37/149 [======>.......................] - ETA: 1:07 - loss: 0.8258 - accuracy: 0.6081(4, 256)\n",
      " 38/149 [======>.......................] - ETA: 1:07 - loss: 0.8185 - accuracy: 0.6118(4, 256)\n",
      " 39/149 [======>.......................] - ETA: 1:06 - loss: 0.8525 - accuracy: 0.5962(4, 256)\n",
      " 40/149 [=======>......................] - ETA: 1:06 - loss: 0.8773 - accuracy: 0.5813(4, 256)\n",
      " 41/149 [=======>......................] - ETA: 1:05 - loss: 0.8932 - accuracy: 0.5671(4, 256)\n",
      " 42/149 [=======>......................] - ETA: 1:04 - loss: 0.8820 - accuracy: 0.5714(4, 256)\n",
      " 43/149 [=======>......................] - ETA: 1:04 - loss: 0.8667 - accuracy: 0.5814(4, 256)\n",
      " 44/149 [=======>......................] - ETA: 1:03 - loss: 0.8675 - accuracy: 0.5739(4, 256)\n",
      " 45/149 [========>.....................] - ETA: 1:03 - loss: 0.8554 - accuracy: 0.5833(4, 256)\n",
      " 46/149 [========>.....................] - ETA: 1:02 - loss: 0.8582 - accuracy: 0.5707(4, 256)\n",
      " 47/149 [========>.....................] - ETA: 1:01 - loss: 0.8565 - accuracy: 0.5638(4, 256)\n",
      " 48/149 [========>.....................] - ETA: 1:01 - loss: 0.8561 - accuracy: 0.5521(4, 256)\n",
      " 49/149 [========>.....................] - ETA: 1:00 - loss: 0.8495 - accuracy: 0.5561(4, 256)\n",
      " 50/149 [=========>....................] - ETA: 1:00 - loss: 0.8447 - accuracy: 0.5600(4, 256)\n",
      " 51/149 [=========>....................] - ETA: 59s - loss: 0.8400 - accuracy: 0.5686 (4, 256)\n",
      " 52/149 [=========>....................] - ETA: 58s - loss: 0.8339 - accuracy: 0.5769(4, 256)\n",
      " 53/149 [=========>....................] - ETA: 58s - loss: 0.8273 - accuracy: 0.5849(4, 256)\n",
      " 54/149 [=========>....................] - ETA: 57s - loss: 0.8255 - accuracy: 0.5880(4, 256)\n",
      " 55/149 [==========>...................] - ETA: 56s - loss: 0.8264 - accuracy: 0.5818(4, 256)\n",
      " 56/149 [==========>...................] - ETA: 56s - loss: 0.8256 - accuracy: 0.5759(4, 256)\n",
      " 57/149 [==========>...................] - ETA: 55s - loss: 0.8266 - accuracy: 0.5658(4, 256)\n",
      " 58/149 [==========>...................] - ETA: 55s - loss: 0.8297 - accuracy: 0.5560(4, 256)\n",
      " 59/149 [==========>...................] - ETA: 54s - loss: 0.8273 - accuracy: 0.5551(4, 256)\n",
      " 60/149 [===========>..................] - ETA: 53s - loss: 0.8250 - accuracy: 0.5542(4, 256)\n",
      " 61/149 [===========>..................] - ETA: 53s - loss: 0.8237 - accuracy: 0.5492(4, 256)\n",
      " 62/149 [===========>..................] - ETA: 52s - loss: 0.8161 - accuracy: 0.5565(4, 256)\n",
      " 63/149 [===========>..................] - ETA: 52s - loss: 0.8192 - accuracy: 0.5476(4, 256)\n",
      " 64/149 [===========>..................] - ETA: 51s - loss: 0.8207 - accuracy: 0.5391(4, 256)\n",
      " 65/149 [============>.................] - ETA: 50s - loss: 0.8223 - accuracy: 0.5308(4, 256)\n",
      " 66/149 [============>.................] - ETA: 50s - loss: 0.8221 - accuracy: 0.5227(4, 256)\n",
      " 67/149 [============>.................] - ETA: 49s - loss: 0.8182 - accuracy: 0.5224(4, 256)\n",
      " 68/149 [============>.................] - ETA: 49s - loss: 0.8178 - accuracy: 0.5184(4, 256)\n",
      " 69/149 [============>.................] - ETA: 48s - loss: 0.8155 - accuracy: 0.5217(4, 256)\n",
      " 70/149 [=============>................] - ETA: 47s - loss: 0.8132 - accuracy: 0.5250(4, 256)\n",
      " 71/149 [=============>................] - ETA: 47s - loss: 0.8098 - accuracy: 0.5282(4, 256)\n",
      " 72/149 [=============>................] - ETA: 46s - loss: 0.8065 - accuracy: 0.5347(4, 256)\n",
      " 73/149 [=============>................] - ETA: 46s - loss: 0.8038 - accuracy: 0.5377(4, 256)\n",
      " 74/149 [=============>................] - ETA: 45s - loss: 0.8015 - accuracy: 0.5372(4, 256)\n",
      " 75/149 [==============>...............] - ETA: 44s - loss: 0.8017 - accuracy: 0.5367(4, 256)\n",
      " 76/149 [==============>...............] - ETA: 44s - loss: 0.7978 - accuracy: 0.5428(4, 256)\n",
      " 77/149 [==============>...............] - ETA: 43s - loss: 0.7938 - accuracy: 0.5487(4, 256)\n",
      " 78/149 [==============>...............] - ETA: 43s - loss: 0.7909 - accuracy: 0.5481(4, 256)\n",
      " 79/149 [==============>...............] - ETA: 42s - loss: 0.7866 - accuracy: 0.5538(4, 256)\n",
      " 80/149 [===============>..............] - ETA: 41s - loss: 0.7835 - accuracy: 0.5562(4, 256)\n",
      " 81/149 [===============>..............] - ETA: 41s - loss: 0.7799 - accuracy: 0.5617(4, 256)\n",
      " 82/149 [===============>..............] - ETA: 40s - loss: 0.7756 - accuracy: 0.5671(4, 256)\n",
      " 83/149 [===============>..............] - ETA: 39s - loss: 0.7742 - accuracy: 0.5693(4, 256)\n",
      " 84/149 [===============>..............] - ETA: 39s - loss: 0.7698 - accuracy: 0.5744(4, 256)\n",
      " 85/149 [================>.............] - ETA: 38s - loss: 0.7675 - accuracy: 0.5765(4, 256)\n",
      " 86/149 [================>.............] - ETA: 38s - loss: 0.7676 - accuracy: 0.5698(4, 256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 87/149 [================>.............] - ETA: 37s - loss: 0.7660 - accuracy: 0.5718(4, 256)\n",
      " 88/149 [================>.............] - ETA: 36s - loss: 0.7625 - accuracy: 0.5767(4, 256)\n",
      " 89/149 [================>.............] - ETA: 36s - loss: 0.7587 - accuracy: 0.5815(4, 256)\n",
      " 90/149 [=================>............] - ETA: 35s - loss: 0.7542 - accuracy: 0.5861(4, 256)\n",
      " 91/149 [=================>............] - ETA: 35s - loss: 0.7493 - accuracy: 0.5907(4, 256)\n",
      " 92/149 [=================>............] - ETA: 34s - loss: 0.7471 - accuracy: 0.5924(4, 256)\n",
      " 93/149 [=================>............] - ETA: 33s - loss: 0.7448 - accuracy: 0.5941(4, 256)\n",
      " 94/149 [=================>............] - ETA: 33s - loss: 0.7427 - accuracy: 0.5957(4, 256)\n",
      " 95/149 [==================>...........] - ETA: 32s - loss: 0.7386 - accuracy: 0.6000(4, 256)\n",
      " 96/149 [==================>...........] - ETA: 32s - loss: 0.7353 - accuracy: 0.6042(4, 256)\n",
      " 97/149 [==================>...........] - ETA: 31s - loss: 0.7306 - accuracy: 0.6082(4, 256)\n",
      " 98/149 [==================>...........] - ETA: 30s - loss: 0.7293 - accuracy: 0.6097(4, 256)\n",
      " 99/149 [==================>...........] - ETA: 30s - loss: 0.7334 - accuracy: 0.6035(4, 256)\n",
      "100/149 [===================>..........] - ETA: 29s - loss: 0.7304 - accuracy: 0.6050(4, 256)\n",
      "101/149 [===================>..........] - ETA: 29s - loss: 0.7255 - accuracy: 0.6089(4, 256)\n",
      "102/149 [===================>..........] - ETA: 28s - loss: 0.7206 - accuracy: 0.6127(4, 256)\n",
      "103/149 [===================>..........] - ETA: 27s - loss: 0.7216 - accuracy: 0.6117(4, 256)\n",
      "104/149 [===================>..........] - ETA: 27s - loss: 0.7207 - accuracy: 0.6106(4, 256)\n",
      "105/149 [====================>.........] - ETA: 26s - loss: 0.7238 - accuracy: 0.6048(4, 256)\n",
      "106/149 [====================>.........] - ETA: 26s - loss: 0.7227 - accuracy: 0.6038(4, 256)\n",
      "107/149 [====================>.........] - ETA: 25s - loss: 0.7231 - accuracy: 0.6028(4, 256)\n",
      "108/149 [====================>.........] - ETA: 24s - loss: 0.7234 - accuracy: 0.5995(4, 256)\n",
      "109/149 [====================>.........] - ETA: 24s - loss: 0.7245 - accuracy: 0.5963(4, 256)\n",
      "110/149 [=====================>........] - ETA: 23s - loss: 0.7246 - accuracy: 0.5932(4, 256)\n",
      "111/149 [=====================>........] - ETA: 23s - loss: 0.7236 - accuracy: 0.5923(4, 256)\n",
      "112/149 [=====================>........] - ETA: 22s - loss: 0.7215 - accuracy: 0.5960(4, 256)\n",
      "113/149 [=====================>........] - ETA: 21s - loss: 0.7201 - accuracy: 0.5996(4, 256)\n",
      "114/149 [=====================>........] - ETA: 21s - loss: 0.7178 - accuracy: 0.6009(4, 256)\n",
      "115/149 [======================>.......] - ETA: 20s - loss: 0.7151 - accuracy: 0.6043(4, 256)\n",
      "116/149 [======================>.......] - ETA: 19s - loss: 0.7143 - accuracy: 0.6056(4, 256)\n",
      "117/149 [======================>.......] - ETA: 19s - loss: 0.7139 - accuracy: 0.6047(4, 256)\n",
      "118/149 [======================>.......] - ETA: 18s - loss: 0.7106 - accuracy: 0.6081(4, 256)\n",
      "119/149 [======================>.......] - ETA: 18s - loss: 0.7071 - accuracy: 0.6113(4, 256)\n",
      "120/149 [=======================>......] - ETA: 17s - loss: 0.7035 - accuracy: 0.6146(4, 256)\n",
      "121/149 [=======================>......] - ETA: 16s - loss: 0.7011 - accuracy: 0.6157(4, 256)\n",
      "122/149 [=======================>......] - ETA: 16s - loss: 0.7008 - accuracy: 0.6148(4, 256)\n",
      "123/149 [=======================>......] - ETA: 15s - loss: 0.6962 - accuracy: 0.6179(4, 256)\n",
      "124/149 [=======================>......] - ETA: 15s - loss: 0.6978 - accuracy: 0.6169(4, 256)\n",
      "125/149 [========================>.....] - ETA: 14s - loss: 0.7003 - accuracy: 0.6120(4, 256)\n",
      "126/149 [========================>.....] - ETA: 13s - loss: 0.7064 - accuracy: 0.6071(4, 256)\n",
      "127/149 [========================>.....] - ETA: 13s - loss: 0.7113 - accuracy: 0.6024(4, 256)\n",
      "128/149 [========================>.....] - ETA: 12s - loss: 0.7113 - accuracy: 0.6016(4, 256)\n",
      "129/149 [========================>.....] - ETA: 12s - loss: 0.7151 - accuracy: 0.6008(4, 256)\n",
      "130/149 [=========================>....] - ETA: 11s - loss: 0.7184 - accuracy: 0.5981(4, 256)\n",
      "131/149 [=========================>....] - ETA: 10s - loss: 0.7198 - accuracy: 0.5954(4, 256)\n",
      "132/149 [=========================>....] - ETA: 10s - loss: 0.7165 - accuracy: 0.5985(4, 256)\n",
      "133/149 [=========================>....] - ETA: 9s - loss: 0.7141 - accuracy: 0.5996 (4, 256)\n",
      "134/149 [=========================>....] - ETA: 9s - loss: 0.7130 - accuracy: 0.6007(4, 256)\n",
      "135/149 [==========================>...] - ETA: 8s - loss: 0.7104 - accuracy: 0.6037(4, 256)\n",
      "136/149 [==========================>...] - ETA: 7s - loss: 0.7078 - accuracy: 0.6066(4, 256)\n",
      "137/149 [==========================>...] - ETA: 7s - loss: 0.7064 - accuracy: 0.6077(4, 256)\n",
      "138/149 [==========================>...] - ETA: 6s - loss: 0.7053 - accuracy: 0.6105(4, 256)\n",
      "139/149 [==========================>...] - ETA: 6s - loss: 0.7038 - accuracy: 0.6133(4, 256)\n",
      "140/149 [===========================>..] - ETA: 5s - loss: 0.7019 - accuracy: 0.6161(4, 256)\n",
      "141/149 [===========================>..] - ETA: 4s - loss: 0.6999 - accuracy: 0.6188(4, 256)\n",
      "142/149 [===========================>..] - ETA: 4s - loss: 0.6992 - accuracy: 0.6180(4, 256)\n",
      "143/149 [===========================>..] - ETA: 3s - loss: 0.6973 - accuracy: 0.6206(4, 256)\n",
      "144/149 [===========================>..] - ETA: 3s - loss: 0.6953 - accuracy: 0.6233(4, 256)\n",
      "145/149 [============================>.] - ETA: 2s - loss: 0.6935 - accuracy: 0.6259(4, 256)\n",
      "146/149 [============================>.] - ETA: 1s - loss: 0.6925 - accuracy: 0.6267(4, 256)\n",
      "147/149 [============================>.] - ETA: 1s - loss: 0.6917 - accuracy: 0.6276(4, 256)\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.6893 - accuracy: 0.6301(4, 256)\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.6881 - accuracy: 0.6326(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "149/149 [==============================] - 103s 674ms/step - loss: 0.6881 - accuracy: 0.6326 - val_loss: 0.4799 - val_accuracy: 0.8750\n",
      "Epoch 2/2\n",
      "(4, 256)\n",
      "  1/149 [..............................] - ETA: 1:30 - loss: 0.6328 - accuracy: 0.5000(4, 256)\n",
      "  2/149 [..............................] - ETA: 1:29 - loss: 0.4852 - accuracy: 0.7500(4, 256)\n",
      "  3/149 [..............................] - ETA: 1:29 - loss: 0.4648 - accuracy: 0.7500(4, 256)\n",
      "  4/149 [..............................] - ETA: 1:28 - loss: 0.4596 - accuracy: 0.7500(4, 256)\n",
      "  5/149 [>.............................] - ETA: 1:28 - loss: 0.4665 - accuracy: 0.7500(4, 256)\n",
      "  6/149 [>.............................] - ETA: 1:27 - loss: 0.4575 - accuracy: 0.7917(4, 256)\n",
      "  7/149 [>.............................] - ETA: 1:27 - loss: 0.4357 - accuracy: 0.8214(4, 256)\n",
      "  8/149 [>.............................] - ETA: 1:26 - loss: 0.4494 - accuracy: 0.8125(4, 256)\n",
      "  9/149 [>.............................] - ETA: 1:26 - loss: 0.4256 - accuracy: 0.8333(4, 256)\n",
      " 10/149 [=>............................] - ETA: 1:25 - loss: 0.4264 - accuracy: 0.8250(4, 256)\n",
      " 11/149 [=>............................] - ETA: 1:24 - loss: 0.4407 - accuracy: 0.8182(4, 256)\n",
      " 12/149 [=>............................] - ETA: 1:24 - loss: 0.4420 - accuracy: 0.8125(4, 256)\n",
      " 13/149 [=>............................] - ETA: 1:23 - loss: 0.4637 - accuracy: 0.8077(4, 256)\n",
      " 14/149 [=>............................] - ETA: 1:23 - loss: 0.4747 - accuracy: 0.7857(4, 256)\n",
      " 15/149 [==>...........................] - ETA: 1:22 - loss: 0.4589 - accuracy: 0.8000(4, 256)\n",
      " 16/149 [==>...........................] - ETA: 1:22 - loss: 0.4596 - accuracy: 0.8125(4, 256)\n",
      " 17/149 [==>...........................] - ETA: 1:21 - loss: 0.4644 - accuracy: 0.8235(4, 256)\n",
      " 18/149 [==>...........................] - ETA: 1:21 - loss: 0.4809 - accuracy: 0.8056(4, 256)\n",
      " 19/149 [==>...........................] - ETA: 1:20 - loss: 0.4928 - accuracy: 0.7895(4, 256)\n",
      " 20/149 [===>..........................] - ETA: 1:20 - loss: 0.4878 - accuracy: 0.8000(4, 256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21/149 [===>..........................] - ETA: 1:19 - loss: 0.4774 - accuracy: 0.8095(4, 256)\n",
      " 22/149 [===>..........................] - ETA: 1:23 - loss: 0.4712 - accuracy: 0.8182(4, 256)\n",
      " 23/149 [===>..........................] - ETA: 1:23 - loss: 0.4625 - accuracy: 0.8261(4, 256)\n",
      " 24/149 [===>..........................] - ETA: 1:22 - loss: 0.4552 - accuracy: 0.8333(4, 256)\n",
      " 25/149 [====>.........................] - ETA: 1:22 - loss: 0.4592 - accuracy: 0.8300(4, 256)\n",
      " 26/149 [====>.........................] - ETA: 1:22 - loss: 0.4483 - accuracy: 0.8365(4, 256)\n",
      " 27/149 [====>.........................] - ETA: 1:21 - loss: 0.4453 - accuracy: 0.8333(4, 256)\n",
      " 28/149 [====>.........................] - ETA: 17:10 - loss: 0.4367 - accuracy: 0.8393(4, 256)\n",
      " 29/149 [====>.........................] - ETA: 16:29 - loss: 0.4250 - accuracy: 0.8448(4, 256)\n",
      " 30/149 [=====>........................] - ETA: 15:50 - loss: 0.4153 - accuracy: 0.8500(4, 256)\n",
      " 31/149 [=====>........................] - ETA: 15:13 - loss: 0.4058 - accuracy: 0.8548(4, 256)\n",
      " 32/149 [=====>........................] - ETA: 14:38 - loss: 0.4139 - accuracy: 0.8516(4, 256)\n",
      " 33/149 [=====>........................] - ETA: 14:06 - loss: 0.4122 - accuracy: 0.8485(4, 256)\n",
      " 34/149 [=====>........................] - ETA: 13:35 - loss: 0.4157 - accuracy: 0.8456(4, 256)\n",
      " 35/149 [======>.......................] - ETA: 13:07 - loss: 0.4512 - accuracy: 0.8286(4, 256)\n",
      " 36/149 [======>.......................] - ETA: 12:40 - loss: 0.4842 - accuracy: 0.8056(4, 256)\n",
      " 37/149 [======>.......................] - ETA: 12:14 - loss: 0.5175 - accuracy: 0.7905(4, 256)\n",
      " 38/149 [======>.......................] - ETA: 11:50 - loss: 0.5064 - accuracy: 0.7961(4, 256)\n",
      " 39/149 [======>.......................] - ETA: 11:27 - loss: 0.5210 - accuracy: 0.7821(4, 256)\n",
      " 40/149 [=======>......................] - ETA: 11:05 - loss: 0.5314 - accuracy: 0.7688(4, 256)\n",
      " 41/149 [=======>......................] - ETA: 10:44 - loss: 0.5445 - accuracy: 0.7561(4, 256)\n",
      " 42/149 [=======>......................] - ETA: 10:24 - loss: 0.5406 - accuracy: 0.7560(4, 256)\n",
      " 43/149 [=======>......................] - ETA: 10:05 - loss: 0.5317 - accuracy: 0.7616(4, 256)\n",
      " 44/149 [=======>......................] - ETA: 14:39 - loss: 0.5265 - accuracy: 0.7670(4, 256)\n",
      " 45/149 [========>.....................] - ETA: 14:13 - loss: 0.5227 - accuracy: 0.7722(4, 256)\n",
      " 46/149 [========>.....................] - ETA: 13:48 - loss: 0.5190 - accuracy: 0.7717(4, 256)\n",
      " 47/149 [========>.....................] - ETA: 13:23 - loss: 0.5182 - accuracy: 0.7713(4, 256)\n",
      " 48/149 [========>.....................] - ETA: 13:00 - loss: 0.5184 - accuracy: 0.7708(4, 256)\n",
      " 49/149 [========>.....................] - ETA: 12:37 - loss: 0.5170 - accuracy: 0.7704(4, 256)\n",
      " 50/149 [=========>....................] - ETA: 12:16 - loss: 0.5134 - accuracy: 0.7750(4, 256)\n",
      " 51/149 [=========>....................] - ETA: 11:55 - loss: 0.5070 - accuracy: 0.7794(4, 256)\n",
      " 52/149 [=========>....................] - ETA: 11:35 - loss: 0.5024 - accuracy: 0.7837(4, 256)\n",
      " 53/149 [=========>....................] - ETA: 11:16 - loss: 0.4983 - accuracy: 0.7877(4, 256)\n",
      " 54/149 [=========>....................] - ETA: 10:57 - loss: 0.4949 - accuracy: 0.7917(4, 256)\n",
      " 55/149 [==========>...................] - ETA: 10:39 - loss: 0.5061 - accuracy: 0.7818(4, 256)\n",
      " 56/149 [==========>...................] - ETA: 10:22 - loss: 0.5067 - accuracy: 0.7768(4, 256)\n",
      " 57/149 [==========>...................] - ETA: 10:06 - loss: 0.5134 - accuracy: 0.7719(4, 256)\n",
      " 58/149 [==========>...................] - ETA: 9:49 - loss: 0.5234 - accuracy: 0.7629 (4, 256)\n",
      " 59/149 [==========>...................] - ETA: 9:34 - loss: 0.5268 - accuracy: 0.7542(4, 256)\n",
      " 60/149 [===========>..................] - ETA: 9:19 - loss: 0.5291 - accuracy: 0.7500(4, 256)\n",
      " 61/149 [===========>..................] - ETA: 9:04 - loss: 0.5281 - accuracy: 0.7500(4, 256)\n",
      " 62/149 [===========>..................] - ETA: 8:50 - loss: 0.5285 - accuracy: 0.7460(4, 256)\n",
      " 63/149 [===========>..................] - ETA: 8:37 - loss: 0.5277 - accuracy: 0.7460(4, 256)\n",
      " 64/149 [===========>..................] - ETA: 8:23 - loss: 0.5279 - accuracy: 0.7422(4, 256)\n",
      " 65/149 [============>.................] - ETA: 8:10 - loss: 0.5327 - accuracy: 0.7346(4, 256)\n",
      " 66/149 [============>.................] - ETA: 7:58 - loss: 0.5347 - accuracy: 0.7311(4, 256)\n",
      " 67/149 [============>.................] - ETA: 7:46 - loss: 0.5321 - accuracy: 0.7351(4, 256)\n",
      " 68/149 [============>.................] - ETA: 7:34 - loss: 0.5279 - accuracy: 0.7390(4, 256)\n",
      " 69/149 [============>.................] - ETA: 7:41 - loss: 0.5244 - accuracy: 0.7428(4, 256)\n",
      " 70/149 [=============>................] - ETA: 7:29 - loss: 0.5205 - accuracy: 0.7464(4, 256)\n",
      " 71/149 [=============>................] - ETA: 7:18 - loss: 0.5211 - accuracy: 0.7465(4, 256)\n",
      " 72/149 [=============>................] - ETA: 7:07 - loss: 0.5208 - accuracy: 0.7465(4, 256)\n",
      " 73/149 [=============>................] - ETA: 6:56 - loss: 0.5180 - accuracy: 0.7500(4, 256)\n",
      " 74/149 [=============>................] - ETA: 6:46 - loss: 0.5239 - accuracy: 0.7432(4, 256)\n",
      " 75/149 [==============>...............] - ETA: 6:36 - loss: 0.5267 - accuracy: 0.7400(4, 256)\n",
      " 76/149 [==============>...............] - ETA: 6:26 - loss: 0.5279 - accuracy: 0.7368(4, 256)\n",
      " 77/149 [==============>...............] - ETA: 6:16 - loss: 0.5244 - accuracy: 0.7403(4, 256)\n",
      " 78/149 [==============>...............] - ETA: 6:06 - loss: 0.5238 - accuracy: 0.7404(4, 256)\n",
      " 79/149 [==============>...............] - ETA: 5:57 - loss: 0.5206 - accuracy: 0.7405(4, 256)\n",
      " 80/149 [===============>..............] - ETA: 5:48 - loss: 0.5178 - accuracy: 0.7406(4, 256)\n",
      " 81/149 [===============>..............] - ETA: 5:39 - loss: 0.5154 - accuracy: 0.7438(4, 256)\n",
      " 82/149 [===============>..............] - ETA: 5:31 - loss: 0.5121 - accuracy: 0.7470(4, 256)\n",
      " 83/149 [===============>..............] - ETA: 5:22 - loss: 0.5095 - accuracy: 0.7500(4, 256)\n",
      " 84/149 [===============>..............] - ETA: 5:14 - loss: 0.5081 - accuracy: 0.7500(4, 256)\n",
      " 85/149 [================>.............] - ETA: 5:06 - loss: 0.5041 - accuracy: 0.7529(4, 256)\n",
      " 86/149 [================>.............] - ETA: 4:58 - loss: 0.5040 - accuracy: 0.7529(4, 256)\n",
      " 87/149 [================>.............] - ETA: 4:50 - loss: 0.5049 - accuracy: 0.7500(4, 256)\n",
      " 88/149 [================>.............] - ETA: 4:43 - loss: 0.5034 - accuracy: 0.7528(4, 256)\n",
      " 89/149 [================>.............] - ETA: 4:36 - loss: 0.5002 - accuracy: 0.7556(4, 256)\n",
      " 90/149 [=================>............] - ETA: 4:28 - loss: 0.4976 - accuracy: 0.7583(4, 256)\n",
      " 91/149 [=================>............] - ETA: 4:21 - loss: 0.4939 - accuracy: 0.7610(4, 256)\n",
      " 92/149 [=================>............] - ETA: 4:14 - loss: 0.4900 - accuracy: 0.7636(4, 256)\n",
      " 93/149 [=================>............] - ETA: 4:08 - loss: 0.4925 - accuracy: 0.7608(4, 256)\n",
      " 94/149 [=================>............] - ETA: 4:01 - loss: 0.4897 - accuracy: 0.7633(4, 256)\n",
      " 95/149 [==================>...........] - ETA: 3:54 - loss: 0.4907 - accuracy: 0.7632(4, 256)\n",
      " 96/149 [==================>...........] - ETA: 3:48 - loss: 0.4882 - accuracy: 0.7656(4, 256)\n",
      " 97/149 [==================>...........] - ETA: 3:42 - loss: 0.4845 - accuracy: 0.7680(4, 256)\n",
      " 98/149 [==================>...........] - ETA: 3:36 - loss: 0.4811 - accuracy: 0.7704(4, 256)\n",
      " 99/149 [==================>...........] - ETA: 3:30 - loss: 0.4850 - accuracy: 0.7677(4, 256)\n",
      "100/149 [===================>..........] - ETA: 3:24 - loss: 0.4861 - accuracy: 0.7675(4, 256)\n",
      "101/149 [===================>..........] - ETA: 3:18 - loss: 0.4832 - accuracy: 0.7698(4, 256)\n",
      "102/149 [===================>..........] - ETA: 3:13 - loss: 0.4798 - accuracy: 0.7721(4, 256)\n",
      "103/149 [===================>..........] - ETA: 3:07 - loss: 0.4791 - accuracy: 0.7718(4, 256)\n",
      "104/149 [===================>..........] - ETA: 3:02 - loss: 0.4792 - accuracy: 0.7716(4, 256)\n",
      "105/149 [====================>.........] - ETA: 2:56 - loss: 0.4792 - accuracy: 0.7714(4, 256)\n",
      "106/149 [====================>.........] - ETA: 2:51 - loss: 0.4804 - accuracy: 0.7689(4, 256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/149 [====================>.........] - ETA: 2:46 - loss: 0.4774 - accuracy: 0.7710(4, 256)\n",
      "108/149 [====================>.........] - ETA: 2:40 - loss: 0.4775 - accuracy: 0.7708(4, 256)\n",
      "109/149 [====================>.........] - ETA: 2:35 - loss: 0.4786 - accuracy: 0.7683(4, 256)\n",
      "110/149 [=====================>........] - ETA: 2:30 - loss: 0.4781 - accuracy: 0.7682(4, 256)\n",
      "111/149 [=====================>........] - ETA: 2:41 - loss: 0.4798 - accuracy: 0.7658(4, 256)\n",
      "112/149 [=====================>........] - ETA: 2:36 - loss: 0.4778 - accuracy: 0.7679(4, 256)\n",
      "113/149 [=====================>........] - ETA: 2:31 - loss: 0.4752 - accuracy: 0.7699(4, 256)\n",
      "114/149 [=====================>........] - ETA: 2:25 - loss: 0.4737 - accuracy: 0.7697(4, 256)\n",
      "115/149 [======================>.......] - ETA: 2:20 - loss: 0.4723 - accuracy: 0.7717(4, 256)\n",
      "116/149 [======================>.......] - ETA: 2:15 - loss: 0.4693 - accuracy: 0.7737(4, 256)\n",
      "117/149 [======================>.......] - ETA: 2:10 - loss: 0.4706 - accuracy: 0.7714(4, 256)\n",
      "118/149 [======================>.......] - ETA: 2:05 - loss: 0.4705 - accuracy: 0.7712(4, 256)\n",
      "119/149 [======================>.......] - ETA: 2:00 - loss: 0.4685 - accuracy: 0.7731(4, 256)\n",
      "120/149 [=======================>......] - ETA: 1:55 - loss: 0.4657 - accuracy: 0.7750(4, 256)\n",
      "121/149 [=======================>......] - ETA: 1:50 - loss: 0.4635 - accuracy: 0.7769(4, 256)\n",
      "122/149 [=======================>......] - ETA: 1:46 - loss: 0.4618 - accuracy: 0.7787(4, 256)\n",
      "123/149 [=======================>......] - ETA: 1:41 - loss: 0.4631 - accuracy: 0.7764(4, 256)\n",
      "124/149 [=======================>......] - ETA: 1:37 - loss: 0.4619 - accuracy: 0.7782(4, 256)\n",
      "125/149 [========================>.....] - ETA: 1:32 - loss: 0.4636 - accuracy: 0.7780(4, 256)\n",
      "126/149 [========================>.....] - ETA: 1:28 - loss: 0.4666 - accuracy: 0.7738(4, 256)\n",
      "127/149 [========================>.....] - ETA: 1:23 - loss: 0.4682 - accuracy: 0.7717(4, 256)\n",
      "128/149 [========================>.....] - ETA: 1:19 - loss: 0.4705 - accuracy: 0.7695(4, 256)\n",
      "129/149 [========================>.....] - ETA: 1:15 - loss: 0.4704 - accuracy: 0.7694(4, 256)\n",
      "130/149 [=========================>....] - ETA: 1:10 - loss: 0.4759 - accuracy: 0.7654(4, 256)\n",
      "131/149 [=========================>....] - ETA: 1:08 - loss: 0.4750 - accuracy: 0.7672(4, 256)\n",
      "132/149 [=========================>....] - ETA: 1:04 - loss: 0.4746 - accuracy: 0.7689(4, 256)\n",
      "133/149 [=========================>....] - ETA: 1:00 - loss: 0.4732 - accuracy: 0.7688(4, 256)\n",
      "134/149 [=========================>....] - ETA: 56s - loss: 0.4715 - accuracy: 0.7705 (4, 256)\n",
      "135/149 [==========================>...] - ETA: 52s - loss: 0.4709 - accuracy: 0.7722(4, 256)\n",
      "136/149 [==========================>...] - ETA: 48s - loss: 0.4683 - accuracy: 0.7739(4, 256)\n",
      "137/149 [==========================>...] - ETA: 44s - loss: 0.4676 - accuracy: 0.7755(4, 256)\n",
      "138/149 [==========================>...] - ETA: 40s - loss: 0.4689 - accuracy: 0.7736(4, 256)\n",
      "139/149 [==========================>...] - ETA: 36s - loss: 0.4681 - accuracy: 0.7752(4, 256)\n",
      "140/149 [===========================>..] - ETA: 32s - loss: 0.4694 - accuracy: 0.7750(4, 256)\n",
      "141/149 [===========================>..] - ETA: 28s - loss: 0.4678 - accuracy: 0.7766(4, 256)\n",
      "142/149 [===========================>..] - ETA: 24s - loss: 0.4667 - accuracy: 0.7782(4, 256)\n",
      "143/149 [===========================>..] - ETA: 21s - loss: 0.4664 - accuracy: 0.7780(4, 256)\n",
      "144/149 [===========================>..] - ETA: 17s - loss: 0.4648 - accuracy: 0.7795(4, 256)\n",
      "145/149 [============================>.] - ETA: 14s - loss: 0.4636 - accuracy: 0.7810(4, 256)\n",
      "146/149 [============================>.] - ETA: 10s - loss: 0.4628 - accuracy: 0.7808(4, 256)\n",
      "147/149 [============================>.] - ETA: 6s - loss: 0.4644 - accuracy: 0.7789 (4, 256)\n",
      "148/149 [============================>.] - ETA: 3s - loss: 0.4630 - accuracy: 0.7804(4, 256)\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4622 - accuracy: 0.7819(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "(4, 256)\n",
      "149/149 [==============================] - 518s 3s/step - loss: 0.4622 - accuracy: 0.7819 - val_loss: 0.2848 - val_accuracy: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x36c0af010>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 epochs takes ~ 1.5 - 2 minutes on Raj's computer\n",
    "model_.fit(\n",
    "    train_data,\n",
    "    epochs=2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    steps_per_epoch=len(train_x)//BATCH_SIZE,\n",
    "    validation_data=test_data,\n",
    "    validation_steps=BATCH_SIZE*4,\n",
    "    validation_batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "of1cKkNACWjH"
   },
   "source": [
    "TODO: Now train the model on the other two files present in the data folder and report your results (make sure that you train your model from scratch). You will want to experiment with different `MAX_LENGTH`s, `batch_size`s, the number of dense layers you have, the number of hidden units per layer.\n",
    "\n",
    "In general, the more layers corresponds to the more levels of abstract information that your model will be able to extract/represent. The more hidden units (the \"wider\") your network has, the more information it will be able to memorize.\n",
    "\n",
    "4. Using the \"base\" model settings (the default values we've set for you), record the `val_accuracy` after 2 epochs for each training data set:\n",
    "    1. imdb: __YOUR ANSWER HERE__\n",
    "    2. amazon: __YOUR ANSWER HERE__\n",
    "    3. yelp: __YOUR ANSWER HERE__\n",
    "\n",
    "5. Choose one dataset. For at least 4 different combinations of settings, record the following five pieces of information:\n",
    "    1. training data (imdb, amazon, yelp) (this will be the same for all 4 combinations)\n",
    "    2. epochs\n",
    "    3. dimensions of your network (# of layers, # of hidden units per layer) — this is the part after the bert model\n",
    "    4. `MAX_LENGTH` value\n",
    "    5. `batch_size` value\n",
    "    6. time to train\n",
    "    7. `val_accuracy` in the end\n",
    "\n",
    "__YOUR RESULTS HERE (format these nicely!)__\n",
    "\n",
    "\n",
    "6. What did your experiments show you? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
